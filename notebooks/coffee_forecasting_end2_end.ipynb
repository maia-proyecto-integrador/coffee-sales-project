{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e2da1fd",
   "metadata": {},
   "source": [
    "\n",
    "# Coffee Demand Forecasting — End-to-End (v2.5)\n",
    "\n",
    "**Novedades v2.5**  \n",
    "- Bugfix en `rolling_origins` (slicing inverso).  \n",
    "- **Prophet**: regressors futuros correctamente provistos por `product` (sin NaN).  \n",
    "- **LGBM**: opción para objetivos de conteo (`poisson`) y **Tweedie** (rejilla corta de `variance_power`).  \n",
    "- **SARIMAX por producto** (m=7, con exógenas).  \n",
    "- Métricas ampliadas: **MASE** y **cobertura** de intervalos p10–p90.  \n",
    "- Nuevas features: `totals_day_roll_7` (causal), `competitor_sum_t1`, `weekofyear`, víspera/post-feriado.  \n",
    "- Ranking por **producto×h** y **ensemble** por producto (mejor de cada familia).\n",
    "\n",
    "> Ejecuta las celdas en orden. Deja `DATA_PATH` y `INDEX_PATH` apuntando a tus archivos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3089fe8-08ff-4ad6-9d44-376b35277d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras: 3.4.1 | Backend: torch | Torch: 2.2.2+cpu\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"   \n",
    "import keras, torch\n",
    "print(\"Keras:\", keras.__version__, \"| Backend:\", keras.config.backend(), \"| Torch:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a15eb12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: C:\\Users\\Julian\n",
      "DATA_PATH: D:\\Julian\\Estudio\\Maestria Inteligencia Artificial\\Materias\\PROYECTO - DESARROLLO Y DESPLIEGUE DE SOLUCIONES\\MICRO - PROYECTOS\\REPOSITORIO\\coffee-sales-project\\data\\processed\\coffee_ml_features.csv\n",
      "INDEX_PATH: D:\\Julian\\Estudio\\Maestria Inteligencia Artificial\\Materias\\PROYECTO - DESARROLLO Y DESPLIEGUE DE SOLUCIONES\\MICRO - PROYECTOS\\REPOSITORIO\\coffee-sales-project\\data\\clean\\index_1.csv\n",
      "RESULTS_DIR: C:\\Users\\Julian\\results\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "# !pip install pandas numpy lightgbm prophet scikit-learn statsmodels pmdarima --quiet\n",
    "\n",
    "from pathlib import Path\n",
    "import os, json, sys, platform, time, hashlib, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "#Paths\n",
    "DATA_PATH = \"D:\\Julian\\Estudio\\Maestria Inteligencia Artificial\\Materias\\PROYECTO - DESARROLLO Y DESPLIEGUE DE SOLUCIONES\\MICRO - PROYECTOS\\REPOSITORIO\\coffee-sales-project\\data\\processed\\coffee_ml_features.csv\"    # diario×producto\n",
    "INDEX_PATH = \"D:\\Julian\\Estudio\\Maestria Inteligencia Artificial\\Materias\\PROYECTO - DESARROLLO Y DESPLIEGUE DE SOLUCIONES\\MICRO - PROYECTOS\\REPOSITORIO\\coffee-sales-project\\data\\clean\\index_1.csv\"              # transaccional con clima+festivos (opcional)\n",
    "\n",
    "#Parámetros generales\n",
    "TARGET = \"transactions\"\n",
    "HORIZON = 7\n",
    "N_ORIGINS = 4\n",
    "MIN_TRAIN_DAYS = 150                    \n",
    "TOPK_IMP = 40                           \n",
    "\n",
    "# Opciones de modelado\n",
    "USE_LOG1P_TARGET = False\n",
    "LGBM_OBJECTIVE = \"auto\"   # \"auto\" | \"poisson\" | \"tweedie\"\n",
    "TWEEDIE_POWERS = [1.1, 1.3, 1.5]\n",
    "CAP_OUTLIERS = False\n",
    "OUTLIER_Q = 0.995\n",
    "\n",
    "# Calendario y contexto\n",
    "USE_RICH_CALENDAR = True\n",
    "UA_HOLIDAYS_PATH = None                 \n",
    "ADD_BUSINESS_AGGREGATES = True\n",
    "\n",
    "# Merge desde index_1.csv (clima + festivo)\n",
    "USE_INDEX_WEATHER_HOLIDAYS = True\n",
    "WEATHER_AGG = {\n",
    "    \"wx_temperature_2m\": \"mean\",\n",
    "    \"wx_precipitation\": \"sum\",\n",
    "    \"wx_cloudcover\": \"mean\"\n",
    "}\n",
    "HOLIDAY_COL = \"is_holiday\"              \n",
    "\n",
    "# Prophet regresores\n",
    "PROPHET_USE_REGRESSORS = True\n",
    "\n",
    "# Semilla reproducible\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "#Directorio de resultados\n",
    "RESULTS_DIR = Path(\"results\")\n",
    "RESULTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"DATA_PATH:\", Path(DATA_PATH).resolve())\n",
    "print(\"INDEX_PATH:\", Path(INDEX_PATH).resolve())\n",
    "print(\"RESULTS_DIR:\", RESULTS_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61022a22-c255-41ac-9fa7-3f0fa75048a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "def _normalize_for_forecast_save(df_in: pd.DataFrame, target: str) -> pd.DataFrame:\n",
    "    \"\"\"Estandariza columnas para el ensemble: date, product, h, y, yhat, (p10, p90), model.\"\"\"\n",
    "    df = df_in.copy()\n",
    "    # h como columna\n",
    "    if \"h\" not in df.columns:\n",
    "        if \"index\" in df.columns: df = df.rename(columns={\"index\": \"h\"})\n",
    "        elif isinstance(df.index, pd.MultiIndex) and \"h\" in df.index.names: df = df.reset_index()\n",
    "\n",
    "    # renombrar verdad a 'y' (admite varios sinónimos)\n",
    "    y_candidates = [\"y\", target, \"revenue\", \"units\", \"qty\", \"quantity\", \"sales\", \"target\", \"y_true\"]\n",
    "    y_col = next((c for c in y_candidates if c in df.columns), None)\n",
    "    if y_col is not None and y_col != \"y\":\n",
    "        df = df.rename(columns={y_col: \"y\"})\n",
    "\n",
    "    cols = [c for c in [\"date\",\"product\",\"h\",\"y\",\"yhat\",\"yhat_p10\",\"yhat_p90\",\"model\"] if c in df.columns]\n",
    "    return df[cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6411bfd1-5214-4fbe-8715-5fe2a5bd990e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow OK: 2.14.1\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "#Setup Mlflow\n",
    "import sys, site\n",
    "\"\"\"!{sys.executable} -m pip install --user -U setuptools wheel\n",
    "!{sys.executable} -m pip install --user \"mlflow==2.14.1\"\n",
    "\"\"\"\n",
    "# Asegura que el kernel vea la ruta de user site en esta sesión\n",
    "import sys, site\n",
    "sys.path.insert(0, site.getusersitepackages())\n",
    "\n",
    "import mlflow\n",
    "print(\"MLflow OK:\", mlflow.__version__)\n",
    "\n",
    "\n",
    "import mlflow, json, os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Carpeta local de tracking dentro de results/\n",
    "MLFLOW_DIR = (RESULTS_DIR / \"mlruns\").resolve()\n",
    "mlflow.set_tracking_uri(MLFLOW_DIR.as_uri())     # p.ej. file:///.../results/mlruns\n",
    "mlflow.set_experiment(\"CoffeeForecasting\")\n",
    "\n",
    "\n",
    "def _mlflow_log_run(model_name: str,\n",
    "                    params: dict,\n",
    "                    overall_csv: Path,\n",
    "                    by_h_csv: Path,\n",
    "                    extra_artifacts: list = None,\n",
    "                    tags: dict = None):\n",
    "    \"\"\"\n",
    "    Lee CSV de métricas (overall y por h) y los registra en MLflow.\n",
    "    - model_name: nombre del run (ej. 'lgbm_direct')\n",
    "    - params: dict con hiperparámetros/config\n",
    "    - overall_csv: ruta al *_metrics_overall.csv\n",
    "    - by_h_csv: ruta al *_metrics_by_h.csv (si no tiene 'h', se infiere el step)\n",
    "    - extra_artifacts: rutas extra para adjuntar (forecasts, modelos, etc.)\n",
    "    - tags: dict de etiquetas (ej. {'stage':'train'})\n",
    "    \"\"\"\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        # parámetros \n",
    "        if params:\n",
    "            clean = {}\n",
    "            for k, v in params.items():\n",
    "                if isinstance(v, (int, float, str, bool)) or v is None:\n",
    "                    clean[k] = v\n",
    "                else:\n",
    "                    clean[k] = str(v)\n",
    "            mlflow.log_params(clean)\n",
    "\n",
    "        if tags:\n",
    "            mlflow.set_tags(tags)\n",
    "\n",
    "        # métricas overall\n",
    "        try:\n",
    "            if Path(overall_csv).exists():\n",
    "                ovr = pd.read_csv(overall_csv)\n",
    "                if not ovr.empty:\n",
    "                    row = ovr.iloc[0].to_dict()\n",
    "                    for key in [\"MAE\",\"RMSE\",\"sMAPE\",\"MAPE\",\"COV_p10_p90_%\"]:\n",
    "                        if key in row and pd.notna(row[key]):\n",
    "                            mlflow.log_metric(key, float(row[key]))\n",
    "        except Exception as e:\n",
    "            print(f\"[mlflow] warning leyendo overall '{overall_csv}':\", e)\n",
    "\n",
    "        # métricas por horizonte\n",
    "        try:\n",
    "            if Path(by_h_csv).exists():\n",
    "                byh = pd.read_csv(by_h_csv)\n",
    "                if not byh.empty:\n",
    "                    if \"h\" in byh.columns:\n",
    "                        steps = byh[\"h\"].astype(int).tolist()\n",
    "                    else:\n",
    "                        steps = list(range(1, len(byh)+1))\n",
    "                    for i, (_, r) in enumerate(byh.iterrows()):\n",
    "                        step = steps[i]\n",
    "                        for key in [\"MAE\",\"RMSE\",\"sMAPE\",\"MAPE\",\"COV_p10_p90_%\"]:\n",
    "                            if key in r and pd.notna(r[key]):\n",
    "                                mlflow.log_metric(f\"{key}_by_h\", float(r[key]), step=int(step))\n",
    "        except Exception as e:\n",
    "            print(f\"[mlflow] warning leyendo by_h '{by_h_csv}':\", e)\n",
    "\n",
    "        # artefactos\n",
    "        for p in [overall_csv, by_h_csv]:\n",
    "            p = Path(p)\n",
    "            if p.exists():\n",
    "                mlflow.log_artifact(str(p))\n",
    "        if extra_artifacts:\n",
    "            for p in extra_artifacts:\n",
    "                p = Path(p)\n",
    "                if p.exists():\n",
    "                    mlflow.log_artifact(str(p))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7ab5838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones base: (3104, 39)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>revenue</th>\n",
       "      <th>transactions</th>\n",
       "      <th>avg_price</th>\n",
       "      <th>first_sale_hour</th>\n",
       "      <th>last_sale_hour</th>\n",
       "      <th>avg_sale_hour</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>...</th>\n",
       "      <th>market_share_transactions</th>\n",
       "      <th>product_Americano</th>\n",
       "      <th>product_Americano with Milk</th>\n",
       "      <th>product_Cappuccino</th>\n",
       "      <th>product_Cocoa</th>\n",
       "      <th>product_Cortado</th>\n",
       "      <th>product_Espresso</th>\n",
       "      <th>product_Hot Chocolate</th>\n",
       "      <th>product_Latte</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>28.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.9</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Americano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-03-02</td>\n",
       "      <td>86.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>28.9</td>\n",
       "      <td>12.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>15.666667</td>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Americano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-03-03</td>\n",
       "      <td>28.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.9</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Americano</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  revenue  transactions  avg_price  first_sale_hour  \\\n",
       "0 2024-03-01     28.9           1.0       28.9             15.0   \n",
       "1 2024-03-02     86.7           3.0       28.9             12.0   \n",
       "2 2024-03-03     28.9           1.0       28.9             14.0   \n",
       "\n",
       "   last_sale_hour  avg_sale_hour  year  month  day  ...  \\\n",
       "0            15.0      15.000000  2024      3    1  ...   \n",
       "1            19.0      15.666667  2024      3    2  ...   \n",
       "2            14.0      14.000000  2024      3    3  ...   \n",
       "\n",
       "   market_share_transactions  product_Americano  product_Americano with Milk  \\\n",
       "0                   0.090909               True                        False   \n",
       "1                   0.428571               True                        False   \n",
       "2                   0.100000               True                        False   \n",
       "\n",
       "   product_Cappuccino  product_Cocoa  product_Cortado  product_Espresso  \\\n",
       "0               False          False            False             False   \n",
       "1               False          False            False             False   \n",
       "2               False          False            False             False   \n",
       "\n",
       "   product_Hot Chocolate  product_Latte    product  \n",
       "0                  False          False  Americano  \n",
       "1                  False          False  Americano  \n",
       "2                  False          False  Americano  \n",
       "\n",
       "[3 rows x 39 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#5\n",
    "# Cargar features diario×producto\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "assert \"date\" in df.columns, \"Se requiere columna 'date'\"\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "# Derivar 'product' desde one-hot product_*\n",
    "prod_cols = [c for c in df.columns if c.startswith(\"product_\")]\n",
    "if len(prod_cols) == 0:\n",
    "    raise ValueError(\"No hay columnas product_* (one-hot).\")\n",
    "df[\"product\"] = df[prod_cols].idxmax(axis=1).str.replace(\"product_\", \"\", regex=False)\n",
    "\n",
    "print(\"Dimensiones base:\", df.shape)\n",
    "display(df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b998dc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agregados diarios desde index_1: ['date', 'wx_temperature_2m', 'wx_precipitation', 'wx_cloudcover', 'is_holiday']\n",
      "Dimensiones tras merge: (3104, 43)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>revenue</th>\n",
       "      <th>transactions</th>\n",
       "      <th>avg_price</th>\n",
       "      <th>first_sale_hour</th>\n",
       "      <th>last_sale_hour</th>\n",
       "      <th>avg_sale_hour</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>...</th>\n",
       "      <th>product_Cocoa</th>\n",
       "      <th>product_Cortado</th>\n",
       "      <th>product_Espresso</th>\n",
       "      <th>product_Hot Chocolate</th>\n",
       "      <th>product_Latte</th>\n",
       "      <th>product</th>\n",
       "      <th>wx_temperature_2m</th>\n",
       "      <th>wx_precipitation</th>\n",
       "      <th>wx_cloudcover</th>\n",
       "      <th>is_holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>28.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.9</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Americano</td>\n",
       "      <td>6.272727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.363636</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-03-02</td>\n",
       "      <td>86.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>28.9</td>\n",
       "      <td>12.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>15.666667</td>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Americano</td>\n",
       "      <td>5.557143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.571429</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-03-03</td>\n",
       "      <td>28.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.9</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Americano</td>\n",
       "      <td>3.070000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>99.400000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  revenue  transactions  avg_price  first_sale_hour  \\\n",
       "0 2024-03-01     28.9           1.0       28.9             15.0   \n",
       "1 2024-03-02     86.7           3.0       28.9             12.0   \n",
       "2 2024-03-03     28.9           1.0       28.9             14.0   \n",
       "\n",
       "   last_sale_hour  avg_sale_hour  year  month  day  ...  product_Cocoa  \\\n",
       "0            15.0      15.000000  2024      3    1  ...          False   \n",
       "1            19.0      15.666667  2024      3    2  ...          False   \n",
       "2            14.0      14.000000  2024      3    3  ...          False   \n",
       "\n",
       "   product_Cortado  product_Espresso  product_Hot Chocolate  product_Latte  \\\n",
       "0            False             False                  False          False   \n",
       "1            False             False                  False          False   \n",
       "2            False             False                  False          False   \n",
       "\n",
       "     product  wx_temperature_2m  wx_precipitation  wx_cloudcover  is_holiday  \n",
       "0  Americano           6.272727               0.0      41.363636       False  \n",
       "1  Americano           5.557143               0.0      12.571429       False  \n",
       "2  Americano           3.070000               0.1      99.400000       False  \n",
       "\n",
       "[3 rows x 43 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#6\n",
    "def build_daily_from_index(index_path, holiday_col=\"is_holiday\", weather_agg=None):\n",
    "    index_path = Path(index_path)\n",
    "    if not index_path.exists():\n",
    "        print(\"[Aviso] INDEX_PATH no existe; se omite merge clima/festivo.\")\n",
    "        return None\n",
    "    raw = pd.read_csv(index_path)\n",
    "    if \"date\" not in raw.columns:\n",
    "        raise ValueError(\"index_1.csv debe tener columna 'date'.\")\n",
    "    raw[\"date\"] = pd.to_datetime(raw[\"date\"])\n",
    "    agg_dict = {}\n",
    "    if weather_agg:\n",
    "        for k, v in weather_agg.items():\n",
    "            if k in raw.columns:\n",
    "                agg_dict[k] = v\n",
    "    if holiday_col in raw.columns:\n",
    "        agg_dict[holiday_col] = \"max\"\n",
    "    if not agg_dict:\n",
    "        print(\"[Aviso] No se encontraron columnas de clima/festivo en index_1 para agregar.\")\n",
    "        return None\n",
    "    daily = raw.groupby(\"date\").agg(agg_dict).reset_index()\n",
    "    return daily\n",
    "\n",
    "daily_idx = build_daily_from_index(INDEX_PATH, HOLIDAY_COL, WEATHER_AGG)\n",
    "\n",
    "if daily_idx is not None:\n",
    "    print(\"Agregados diarios desde index_1:\", daily_idx.columns.tolist())\n",
    "    df = df.merge(daily_idx, on=\"date\", how=\"left\")\n",
    "else:\n",
    "    print(\"Sin merge desde index_1 (no disponible o sin columnas útiles).\")\n",
    "\n",
    "print(\"Dimensiones tras merge:\", df.shape)\n",
    "display(df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52bc3e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicados por (date, product): 0\n",
      "Nulos por columna (top 15) tras reindex:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "is_holiday                     56\n",
       "wx_cloudcover                  56\n",
       "wx_precipitation               56\n",
       "wx_temperature_2m              56\n",
       "product_Americano with Milk     0\n",
       "transactions_roll_7             0\n",
       "transactions_roll_30            0\n",
       "transactions_vol_7              0\n",
       "total_daily_transactions        0\n",
       "total_daily_revenue             0\n",
       "market_share_transactions       0\n",
       "product_Americano               0\n",
       "product_Cocoa                   0\n",
       "product_Cappuccino              0\n",
       "transactions_lag_14             0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preflight OK.\n"
     ]
    }
   ],
   "source": [
    "#7\n",
    "\n",
    "dups = df.duplicated(subset=[\"date\",\"product\"]).sum()\n",
    "print(f\"Duplicados por (date, product): {dups}\")\n",
    "if dups > 0:\n",
    "    display(df[df.duplicated(subset=[\"date\",\"product\"], keep=False)].sort_values([\"product\",\"date\"]).head(10))\n",
    "\n",
    "# Reindexar calendario por producto\n",
    "def ensure_complete_calendar(dfin):\n",
    "    out = []\n",
    "    for prod, g in dfin.groupby(\"product\"):\n",
    "        g = g.sort_values(\"date\")\n",
    "        full_idx = pd.date_range(g[\"date\"].min(), g[\"date\"].max(), freq=\"D\")\n",
    "        g = g.set_index(\"date\").reindex(full_idx)\n",
    "        g[\"product\"] = prod\n",
    "        g.index.name = \"date\"\n",
    "        out.append(g.reset_index())\n",
    "    return pd.concat(out, ignore_index=True)\n",
    "\n",
    "df = ensure_complete_calendar(df)\n",
    "print(\"Nulos por columna (top 15) tras reindex:\")\n",
    "display(df.isna().sum().sort_values(ascending=False).head(15))\n",
    "print(\"Preflight OK.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d6a70fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstrucción causal aplicada a:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>window</th>\n",
       "      <th>reconstructed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>transactions_roll_3</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>transactions_roll_7</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>transactions_roll_30</td>\n",
       "      <td>30</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 column  window  reconstructed\n",
       "0   transactions_roll_3       3           True\n",
       "1   transactions_roll_7       7           True\n",
       "2  transactions_roll_30      30           True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAP_OUTLIERS=False → sin capping.\n"
     ]
    }
   ],
   "source": [
    "#8\n",
    "def rebuild_causal_rollings(dfin, ycol=TARGET):\n",
    "    dfin = dfin.sort_values([\"product\",\"date\"]).copy()\n",
    "    cols = [c for c in dfin.columns if c.startswith(f\"{ycol}_roll_\")]\n",
    "    if not cols:\n",
    "        print(\"No hay columnas roll_* del target para reconstruir.\")\n",
    "        return dfin, pd.DataFrame(columns=[\"column\",\"window\",\"reconstructed\"])\n",
    "    log = []\n",
    "    out = []\n",
    "    for prod, g in dfin.groupby(\"product\"):\n",
    "        g = g.sort_values(\"date\").copy()\n",
    "        s = g[ycol].shift(1)  # causal\n",
    "        for rc in cols:\n",
    "            try:\n",
    "                w = int(rc.split(\"_\")[-1])\n",
    "            except:\n",
    "                w = None\n",
    "            if w is None: \n",
    "                continue\n",
    "            g[rc] = s.rolling(w, min_periods=1).mean()\n",
    "            log.append({\"column\": rc, \"window\": w, \"reconstructed\": True})\n",
    "        out.append(g)\n",
    "    log_df = pd.DataFrame(log).drop_duplicates().sort_values([\"window\",\"column\"])\n",
    "    return pd.concat(out, ignore_index=True), log_df\n",
    "\n",
    "df, rebuild_log = rebuild_causal_rollings(df, ycol=TARGET)\n",
    "print(\"Reconstrucción causal aplicada a:\")\n",
    "display(rebuild_log)\n",
    "\n",
    "if CAP_OUTLIERS:\n",
    "    q_map = df.groupby(\"product\")[TARGET].quantile(OUTLIER_Q).rename(\"q_hi\")\n",
    "    df = df.merge(q_map, on=\"product\", how=\"left\")\n",
    "    before = df[TARGET].copy()\n",
    "    df[TARGET] = np.where(df[TARGET] > df[\"q_hi\"], df[\"q_hi\"], df[TARGET])\n",
    "    print(f\"Capping aplicado: {(before != df[TARGET]).sum()} valores.\")\n",
    "    df.drop(columns=[\"q_hi\"], inplace=True)\n",
    "else:\n",
    "    print(\"CAP_OUTLIERS=False → sin capping.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e35ea7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>product</th>\n",
       "      <th>transactions</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>wx_temperature_2m</th>\n",
       "      <th>wx_precipitation</th>\n",
       "      <th>wx_cloudcover</th>\n",
       "      <th>dow</th>\n",
       "      <th>dow_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>totals_day_t1</th>\n",
       "      <th>share_day_t1</th>\n",
       "      <th>totals_day_roll_7</th>\n",
       "      <th>competitor_sum_t1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>Americano</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>6.272727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.363636</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-03-02</td>\n",
       "      <td>Americano</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>5.557143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.571429</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>0.5</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-03-03</td>\n",
       "      <td>Americano</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>3.070000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>99.400000</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-03-04</td>\n",
       "      <td>Americano</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>9.333333</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-03-05</td>\n",
       "      <td>Americano</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>5.622222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.888889</td>\n",
       "      <td>1</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date    product  transactions is_holiday  wx_temperature_2m  \\\n",
       "0 2024-03-01  Americano           1.0      False           6.272727   \n",
       "1 2024-03-02  Americano           3.0      False           5.557143   \n",
       "2 2024-03-03  Americano           1.0      False           3.070000   \n",
       "3 2024-03-04  Americano           0.0      False           6.300000   \n",
       "4 2024-03-05  Americano           0.0      False           5.622222   \n",
       "\n",
       "   wx_precipitation  wx_cloudcover  dow   dow_sin  month_cos  totals_day_t1  \\\n",
       "0               0.0      41.363636    4 -0.433884        0.5            NaN   \n",
       "1               0.0      12.571429    5 -0.974928        0.5           11.0   \n",
       "2               0.1      99.400000    6 -0.781831        0.5            7.0   \n",
       "3               0.0      99.500000    0  0.000000        0.5           10.0   \n",
       "4               0.0      85.888889    1  0.781831        0.5            4.0   \n",
       "\n",
       "   share_day_t1  totals_day_roll_7  competitor_sum_t1  \n",
       "0           NaN                NaN                NaN  \n",
       "1      0.090909          11.000000               10.0  \n",
       "2      0.428571           9.000000                4.0  \n",
       "3      0.100000           9.333333                9.0  \n",
       "4      0.000000           8.000000                4.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#9\n",
    "def add_calendar_rich(dfin, holidays_path=None):\n",
    "    dfo = dfin.copy()\n",
    "    dt = pd.to_datetime(dfo[\"date\"])\n",
    "    # básicos\n",
    "    dfo[\"dow\"] = dt.dt.dayofweek\n",
    "    dfo[\"month\"] = dt.dt.month\n",
    "    dfo[\"weekofyear\"] = dt.dt.isocalendar().week.astype(int)\n",
    "    dfo[\"is_weekend\"] = (dfo[\"dow\"] >= 5).astype(int)\n",
    "    dfo[\"is_month_start\"] = dt.dt.is_month_start.astype(int)\n",
    "    dfo[\"is_month_end\"] = dt.dt.is_month_end.astype(int)\n",
    "    # víspera/post-feriado\n",
    "    if \"is_holiday\" not in dfo.columns:\n",
    "        dfo[\"is_holiday\"] = 0\n",
    "    dfo = dfo.sort_values(\"date\")\n",
    "    dfo[\"is_holiday_prev\"] = dfo[\"is_holiday\"].shift(1).fillna(0).astype(int)\n",
    "    dfo[\"is_holiday_next\"] = dfo[\"is_holiday\"].shift(-1).fillna(0).astype(int)\n",
    "    # cíclicos\n",
    "    dfo[\"dow_sin\"] = np.sin(2*np.pi*dt.dt.dayofweek/7)\n",
    "    dfo[\"dow_cos\"] = np.cos(2*np.pi*dt.dt.dayofweek/7)\n",
    "    dfo[\"month_sin\"] = np.sin(2*np.pi*(dt.dt.month-1)/12)\n",
    "    dfo[\"month_cos\"] = np.cos(2*np.pi*(dt.dt.month-1)/12)\n",
    "    # festivos externos\n",
    "    if holidays_path:\n",
    "        h = pd.read_csv(holidays_path)\n",
    "        h[\"date\"] = pd.to_datetime(h[\"date\"])\n",
    "        h[\"is_holiday_ext\"] = 1\n",
    "        dfo = dfo.merge(h[[\"date\",\"is_holiday_ext\"]], on=\"date\", how=\"left\")\n",
    "        dfo[\"is_holiday_ext\"] = dfo[\"is_holiday_ext\"].fillna(0).astype(int)\n",
    "    return dfo\n",
    "\n",
    "def add_business_aggregates_tminus1(dfin, ycol=TARGET):\n",
    "    dfo = dfin.sort_values([\"date\",\"product\"]).copy()\n",
    "    totals = dfo.groupby(\"date\")[ycol].sum().rename(\"totals_day\")\n",
    "    dfo = dfo.merge(totals, left_on=\"date\", right_index=True, how=\"left\")\n",
    "    dfo[\"share_day\"] = np.where(dfo[\"totals_day\"]>0, dfo[ycol]/dfo[\"totals_day\"], 0.0)\n",
    "    dfo[\"totals_day_t1\"] = dfo.groupby(\"product\")[\"totals_day\"].shift(1)\n",
    "    dfo[\"share_day_t1\"]  = dfo.groupby(\"product\")[\"share_day\"].shift(1)\n",
    "    # roll_7 causal de totales y \"competidor\"\n",
    "    dfo = dfo.sort_values([\"product\",\"date\"])\n",
    "    dfo[\"totals_day_roll_7\"] = dfo.groupby(\"product\")[\"totals_day_t1\"].transform(\n",
    "        lambda s: s.rolling(7, min_periods=1).mean()\n",
    "    )\n",
    "    dfo[\"competitor_sum_t1\"] = dfo[\"totals_day_t1\"] - dfo.groupby(\"product\")[ycol].shift(1)\n",
    "    dfo.drop(columns=[\"totals_day\",\"share_day\"], inplace=True)\n",
    "    return dfo\n",
    "\n",
    "if USE_RICH_CALENDAR:\n",
    "    df = add_calendar_rich(df, UA_HOLIDAYS_PATH)\n",
    "if ADD_BUSINESS_AGGREGATES:\n",
    "    df = add_business_aggregates_tminus1(df, ycol=TARGET)\n",
    "\n",
    "cols_show = [c for c in [\"is_holiday\",\"is_holiday_ext\",\"wx_temperature_2m\",\"wx_precipitation\",\"wx_cloudcover\",\n",
    "                         \"dow\",\"dow_sin\",\"month_cos\",\"totals_day_t1\",\"share_day_t1\",\"totals_day_roll_7\",\"competitor_sum_t1\"] if c in df.columns]\n",
    "display(df.head(5)[[\"date\",\"product\",TARGET]+cols_show])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d037ee8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>revenue</th>\n",
       "      <th>transactions</th>\n",
       "      <th>avg_price</th>\n",
       "      <th>first_sale_hour</th>\n",
       "      <th>last_sale_hour</th>\n",
       "      <th>avg_sale_hour</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>...</th>\n",
       "      <th>dow</th>\n",
       "      <th>weekofyear</th>\n",
       "      <th>is_holiday_prev</th>\n",
       "      <th>is_holiday_next</th>\n",
       "      <th>dow_sin</th>\n",
       "      <th>dow_cos</th>\n",
       "      <th>totals_day_t1</th>\n",
       "      <th>share_day_t1</th>\n",
       "      <th>totals_day_roll_7</th>\n",
       "      <th>competitor_sum_t1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>28.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.9</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-03-02</td>\n",
       "      <td>86.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>28.9</td>\n",
       "      <td>12.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>15.666667</td>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-03-03</td>\n",
       "      <td>28.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.9</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  revenue  transactions  avg_price  first_sale_hour  \\\n",
       "0 2024-03-01     28.9           1.0       28.9             15.0   \n",
       "1 2024-03-02     86.7           3.0       28.9             12.0   \n",
       "2 2024-03-03     28.9           1.0       28.9             14.0   \n",
       "\n",
       "   last_sale_hour  avg_sale_hour  year  month  day  ...  dow  weekofyear  \\\n",
       "0            15.0      15.000000  2024      3    1  ...    4           9   \n",
       "1            19.0      15.666667  2024      3    2  ...    5           9   \n",
       "2            14.0      14.000000  2024      3    3  ...    6           9   \n",
       "\n",
       "   is_holiday_prev  is_holiday_next   dow_sin   dow_cos  totals_day_t1  \\\n",
       "0                0                0 -0.433884 -0.900969            NaN   \n",
       "1                0                0 -0.974928 -0.222521           11.0   \n",
       "2                0                0 -0.781831  0.623490            7.0   \n",
       "\n",
       "   share_day_t1  totals_day_roll_7  competitor_sum_t1  \n",
       "0           NaN                NaN                NaN  \n",
       "1      0.090909               11.0               10.0  \n",
       "2      0.428571                9.0                4.0  \n",
       "\n",
       "[3 rows x 53 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#10\n",
    "def build_causal_features(dfin, ycol=TARGET):\n",
    "    dfin = dfin.sort_values([\"product\",\"date\"]).copy()\n",
    "    out = []\n",
    "    for prod, g in dfin.groupby(\"product\"):\n",
    "        g = g.sort_values(\"date\").copy()\n",
    "        # lags\n",
    "        g[f\"{ycol}_lag_1\"]  = g[ycol].shift(1)\n",
    "        g[f\"{ycol}_lag_7\"]  = g[ycol].shift(7)\n",
    "        g[f\"{ycol}_lag_14\"] = g[ycol].shift(14)\n",
    "        # rollings causales\n",
    "        s = g[ycol].shift(1)\n",
    "        g[f\"{ycol}_roll_3\"]  = s.rolling(3,  min_periods=1).mean()\n",
    "        g[f\"{ycol}_roll_7\"]  = s.rolling(7,  min_periods=1).mean()\n",
    "        g[f\"{ycol}_roll_30\"] = s.rolling(30, min_periods=1).mean()\n",
    "        g[f\"{ycol}_vol_7\"]   = s.rolling(7,  min_periods=1).std()\n",
    "        out.append(g)\n",
    "    return pd.concat(out, ignore_index=True)\n",
    "\n",
    "df = build_causal_features(df, ycol=TARGET)\n",
    "display(df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d09ff807-e8aa-4d88-ae90-605f62a33c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Guardado] df_final -> artifacts/df_final.parquet & df_final.csv\n"
     ]
    }
   ],
   "source": [
    "#11\n",
    "# Persistir dataset final de entrenamiento/inferencia\n",
    "ARTIFACTS_DIR = RESULTS_DIR / \"artifacts\"\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Copia exacta\n",
    "df_final = df.copy()\n",
    "\n",
    "# Dtypes + shape \n",
    "schema = pd.Series({col: str(df_final[col].dtype) for col in df_final.columns})\n",
    "schema.to_csv(ARTIFACTS_DIR / \"df_final_schema.csv\", header=[\"dtype\"])\n",
    "\n",
    "\n",
    "#df_final.to_parquet(ARTIFACTS_DIR / \"df_final.parquet\", index=False)\n",
    "df_final.to_csv(ARTIFACTS_DIR / \"df_final.csv\", index=False)\n",
    "\n",
    "print(\"[Guardado] df_final -> artifacts/df_final.parquet & df_final.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "581aabce-8250-458c-802f-8a79b991f6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#12\n",
    "#Snapshot de datos y features\n",
    "ART_DIR = (RESULTS_DIR / \"artifacts\"); ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# guarda una muestra ligera para inspección rápida\n",
    "df_final.sample(min(1000, len(df_final))).to_csv(ART_DIR / \"df_final_sample.csv\", index=False)\n",
    "df_final.to_parquet(ART_DIR / \"df_final.parquet\")\n",
    "\n",
    "# esquema (nombres y dtypes)\n",
    "pd.DataFrame({\n",
    "    \"column\": df_final.columns,\n",
    "    \"dtype\": [str(t) for t in df_final.dtypes]\n",
    "}).to_csv(ART_DIR / \"df_final_schema.csv\", index=False)\n",
    "\n",
    "# features candidatos si existen\n",
    "try:\n",
    "    pd.Series(candidates, name=\"feature\").to_csv(ART_DIR / \"features_selected.csv\", index=False)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# log en mlflow (usamos cualquier par de CSVs de métricas para completar; por ej. baselines)\n",
    "_mlflow_log_run(\n",
    "    model_name=\"data_snapshot\",\n",
    "    params={\n",
    "        \"n_rows\": int(len(df_final)),\n",
    "        \"n_products\": int(df_final[\"product\"].nunique()),\n",
    "        \"n_days\": int(df_final[\"date\"].nunique()),\n",
    "        \"target\": TARGET\n",
    "    },\n",
    "    overall_csv=RESULTS_DIR / \"baselines_metrics_overall.csv\",\n",
    "    by_h_csv=RESULTS_DIR / \"baselines_metrics_by_h.csv\",\n",
    "    extra_artifacts=[\n",
    "        ART_DIR / \"df_final.parquet\",\n",
    "        ART_DIR / \"df_final_schema.csv\",\n",
    "        ART_DIR / \"df_final_sample.csv\",\n",
    "        ART_DIR / \"features_selected.csv\",\n",
    "    ],\n",
    "    tags={\"stage\": \"data\"}\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0f2f435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas listas.\n"
     ]
    }
   ],
   "source": [
    "#13\n",
    "def mae(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    mask = ~np.isnan(y_true) & ~np.isnan(y_pred)\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return float(np.mean(np.abs(y_true[mask] - y_pred[mask])))\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    mask = ~np.isnan(y_true) & ~np.isnan(y_pred)\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return float(np.sqrt(np.mean((y_true[mask] - y_pred[mask]) ** 2)))\n",
    "\n",
    "def mape(y_true, y_pred, epsilon=1e-6, ignore_zeros=True):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    if ignore_zeros:\n",
    "        mask = np.abs(y_true) > epsilon\n",
    "        if mask.sum() == 0:\n",
    "            return np.nan\n",
    "        return float(np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100.0)\n",
    "    else:\n",
    "        denom = np.where(np.abs(y_true) < epsilon, epsilon, np.abs(y_true))\n",
    "        return float(np.mean(np.abs(y_true - y_pred) / denom) * 100.0)\n",
    "\n",
    "def smape(y_true, y_pred, epsilon=1e-6):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred)).clip(min=epsilon)\n",
    "    return float(np.mean(2.0 * np.abs(y_pred - y_true) / denom) * 100.0)\n",
    "\n",
    "def mase(y_true, y_pred, y_train, m=7, epsilon=1e-6):\n",
    "    # Escala por error sNaive en train\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    denom_series = np.abs(np.asarray(y_train[m:], dtype=float) - np.asarray(y_train[:-m], dtype=float))\n",
    "    d = np.mean(denom_series) if len(denom_series) > 0 else np.nan\n",
    "    num = np.mean(np.abs(y_true - y_pred))\n",
    "    if d is None or np.isnan(d) or d < epsilon:\n",
    "        return np.nan\n",
    "    return float(num / d)\n",
    "\n",
    "def coverage(y_true, y_lo, y_hi):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_lo = np.asarray(y_lo, dtype=float)\n",
    "    y_hi = np.asarray(y_hi, dtype=float)\n",
    "    mask = ~np.isnan(y_true) & ~np.isnan(y_lo) & ~np.isnan(y_hi)\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    inside = (y_true[mask] >= y_lo[mask]) & (y_true[mask] <= y_hi[mask])\n",
    "    return float(np.mean(inside) * 100.0)\n",
    "\n",
    "def summarize_metrics(df_eval, target_col=\"y\"):\n",
    "    out = {}\n",
    "    for h, g in df_eval.groupby(\"h\"):\n",
    "        out[h] = {\n",
    "            \"MAE\": mae(g[target_col], g[\"yhat\"]),\n",
    "            \"RMSE\": rmse(g[target_col], g[\"yhat\"]),\n",
    "            \"MAPE\": mape(g[target_col], g[\"yhat\"], ignore_zeros=True),\n",
    "            \"sMAPE\": smape(g[target_col], g[\"yhat\"]),\n",
    "        }\n",
    "        if {\"yhat_p10\",\"yhat_p90\"}.issubset(g.columns):\n",
    "            out[h][\"COV_p10_p90_%\"] = coverage(g[target_col], g[\"yhat_p10\"], g[\"yhat_p90\"])\n",
    "    overall = {\n",
    "        \"MAE\": mae(df_eval[target_col], df_eval[\"yhat\"]),\n",
    "        \"RMSE\": rmse(df_eval[target_col], df_eval[\"yhat\"]),\n",
    "        \"MAPE\": mape(df_eval[target_col], df_eval[\"yhat\"], ignore_zeros=True),\n",
    "        \"sMAPE\": smape(df_eval[target_col], df_eval[\"yhat\"]),\n",
    "    }\n",
    "    if {\"yhat_p10\",\"yhat_p90\"}.issubset(df_eval.columns):\n",
    "        overall[\"COV_p10_p90_%\"] = coverage(df_eval[target_col], df_eval[\"yhat_p10\"], df_eval[\"yhat_p90\"])\n",
    "    return pd.DataFrame(out).T, overall\n",
    "\n",
    "print(\"Métricas listas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02241194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Timestamp('2025-02-23 00:00:00'),\n",
       "  Timestamp('2025-02-24 00:00:00'),\n",
       "  Timestamp('2025-03-02 00:00:00')),\n",
       " (Timestamp('2025-03-02 00:00:00'),\n",
       "  Timestamp('2025-03-03 00:00:00'),\n",
       "  Timestamp('2025-03-09 00:00:00')),\n",
       " (Timestamp('2025-03-09 00:00:00'),\n",
       "  Timestamp('2025-03-10 00:00:00'),\n",
       "  Timestamp('2025-03-16 00:00:00')),\n",
       " (Timestamp('2025-03-16 00:00:00'),\n",
       "  Timestamp('2025-03-17 00:00:00'),\n",
       "  Timestamp('2025-03-23 00:00:00'))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#14\n",
    "from typing import List, Tuple\n",
    "\n",
    "def rolling_origins(date_index: pd.Series, n_origins: int = 4, horizon: int = 7):\n",
    "    unique_dates = pd.Series(pd.to_datetime(pd.unique(date_index))).sort_values()\n",
    "    anchors = [unique_dates.iloc[-(i+1)*horizon] for i in range(n_origins)][::-1]  # bugfix\n",
    "    splits: List[Tuple[pd.Timestamp, pd.Timestamp, pd.Timestamp]] = []\n",
    "    for anchor in anchors:\n",
    "        train_end = anchor - pd.Timedelta(days=1)\n",
    "        test_start = anchor\n",
    "        test_end = anchor + pd.Timedelta(days=horizon - 1)\n",
    "        splits.append((train_end, test_start, test_end))\n",
    "    return splits\n",
    "\n",
    "splits_demo = rolling_origins(df[\"date\"], n_origins=N_ORIGINS, horizon=HORIZON)\n",
    "splits_demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fccbc60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#15\n",
    "BASELINE_NAMES = [\"naive1\", \"snaive7\", \"ma7\"]\n",
    "\n",
    "def _baseline_predict(train: pd.DataFrame, test_dates: pd.DatetimeIndex, horizon: int, target: str, kind: str):\n",
    "    pieces = []\n",
    "    for prod, g in train.groupby(\"product\"):\n",
    "        g = g.sort_values(\"date\")\n",
    "        if len(g) == 0:\n",
    "            continue\n",
    "        if kind == \"naive1\":\n",
    "            last = g[target].iloc[-1]\n",
    "            preds = [last] * horizon\n",
    "        elif kind == \"snaive7\":\n",
    "            hist = g[target].iloc[-7:].tolist()\n",
    "            if len(hist) < 7:\n",
    "                hist = [g[target].iloc[-1]] * 7\n",
    "            preds = hist\n",
    "        elif kind == \"ma7\":\n",
    "            window = g[target].iloc[-7:]\n",
    "            meanv = float(window.mean()) if len(window) > 0 else float(g[target].iloc[-1])\n",
    "            preds = [meanv] * horizon\n",
    "        else:\n",
    "            raise ValueError(kind)\n",
    "        dfp = pd.DataFrame({\n",
    "            \"date\": test_dates,\n",
    "            \"product\": prod,\n",
    "            \"h\": list(range(1, horizon + 1)),\n",
    "            \"yhat\": preds[:horizon],\n",
    "        })\n",
    "        pieces.append(dfp)\n",
    "    return pd.concat(pieces, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1ff1931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidatas post-filtro: 49\n"
     ]
    }
   ],
   "source": [
    "#16\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "def filter_candidates(dfin, target=TARGET, null_frac_max=0.30):\n",
    "    drop_like = {target, \"date\", \"product\"}\n",
    "    cols = [c for c in dfin.columns if c not in drop_like and not c.startswith(\"y_\")]\n",
    "    cols = [c for c in cols if pd.api.types.is_numeric_dtype(dfin[c])]\n",
    "    kept = []\n",
    "    for c in cols:\n",
    "        null_frac = dfin[c].isna().mean()\n",
    "        if null_frac > null_frac_max: \n",
    "            continue\n",
    "        if dfin[c].nunique(dropna=True) <= 1:\n",
    "            continue\n",
    "        kept.append(c)\n",
    "    return kept\n",
    "\n",
    "candidates = filter_candidates(df, target=TARGET, null_frac_max=0.30)\n",
    "print(\"Candidatas post-filtro:\", len(candidates))\n",
    "\n",
    "def build_direct_labels(dfin, target, H=7):\n",
    "    dfx = dfin.sort_values([\"product\",\"date\"]).copy()\n",
    "    for h in range(1, H+1):\n",
    "        dfx[f\"y_{h}\"] = dfx.groupby(\"product\")[target].shift(-h)\n",
    "    return dfx\n",
    "\n",
    "df_l = build_direct_labels(df, TARGET, H=HORIZON)\n",
    "\n",
    "def _fit_lgbm(X, y, objective=\"auto\", tweedie_power=None):\n",
    "    params = dict(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=31,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "    if objective == \"poisson\":\n",
    "        params.update(objective=\"poisson\")\n",
    "    elif objective == \"tweedie\":\n",
    "        params.update(objective=\"tweedie\")\n",
    "        if tweedie_power is not None:\n",
    "            params.update(tweedie_variance_power=float(tweedie_power))\n",
    "    model = LGBMRegressor(**params)\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "def _fit_lgbm_quantile(X, y, alpha):\n",
    "    model = LGBMRegressor(\n",
    "        objective=\"quantile\", alpha=alpha,\n",
    "        n_estimators=500, learning_rate=0.05,\n",
    "        num_leaves=31, subsample=0.9, colsample_bytree=0.9,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "def lgbm_importance_until(dfin, feats, y_col, cutoff_date):\n",
    "    data = dfin[(dfin[\"date\"] <= cutoff_date)].dropna(subset=[y_col]).copy()\n",
    "    if data.empty:\n",
    "        return pd.DataFrame({\"feature\":[], \"importance\":[]})\n",
    "    X, y = data[feats], data[y_col]\n",
    "    m = LGBMRegressor(n_estimators=300, learning_rate=0.05, num_leaves=31,\n",
    "                      subsample=0.9, colsample_bytree=0.9, random_state=RANDOM_STATE)\n",
    "    m.fit(X, y)\n",
    "    imp = pd.DataFrame({\"feature\": feats, \"importance\": m.feature_importances_})\n",
    "    return imp.sort_values(\"importance\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8274b609-5aeb-49dc-8d4d-16f1a419e52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Features seleccionadas - 49]:\n",
      " - revenue\n",
      " - avg_price\n",
      " - first_sale_hour\n",
      " - last_sale_hour\n",
      " - avg_sale_hour\n",
      " - year\n",
      " - month\n",
      " - day\n",
      " - dayofweek\n",
      " - quarter\n",
      " - week_of_year\n",
      " - is_weekend\n",
      " - is_month_start\n",
      " - is_month_end\n",
      " - month_sin\n",
      " - month_cos\n",
      " - dayofweek_sin\n",
      " - dayofweek_cos\n",
      " - transactions_lag_1\n",
      " - transactions_lag_7\n",
      " - transactions_lag_14\n",
      " - transactions_roll_3\n",
      " - transactions_roll_7\n",
      " - transactions_roll_30\n",
      " - transactions_vol_7\n",
      " - total_daily_transactions\n",
      " - total_daily_revenue\n",
      " - market_share_transactions\n",
      " - product_Americano\n",
      " - product_Americano with Milk\n",
      " - product_Cappuccino\n",
      " - product_Cocoa\n",
      " - product_Cortado\n",
      " - product_Espresso\n",
      " - product_Hot Chocolate\n",
      " - product_Latte\n",
      " - wx_temperature_2m\n",
      " - wx_precipitation\n",
      " - wx_cloudcover\n",
      " - dow\n",
      " - weekofyear\n",
      " - is_holiday_prev\n",
      " - is_holiday_next\n",
      " - dow_sin\n",
      " - dow_cos\n",
      " - totals_day_t1\n",
      " - share_day_t1\n",
      " - totals_day_roll_7\n",
      " - competitor_sum_t1\n",
      "[Guardado] features_selected.csv\n"
     ]
    }
   ],
   "source": [
    "#17 \n",
    "#Inventario de features (candidates) usadas por modelos ML\n",
    "ARTIFACTS_DIR = RESULTS_DIR / \"artifacts\"\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"[Features seleccionadas - {len(candidates)}]:\")\n",
    "for c in candidates:\n",
    "    print(\" -\", c)\n",
    "\n",
    "pd.Series(candidates, name=\"feature\").to_csv(ARTIFACTS_DIR / \"features_selected.csv\", index=False)\n",
    "print(\"[Guardado] features_selected.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880254d3-a4e5-4906-a5cb-4022ecccffb5",
   "metadata": {},
   "source": [
    "## LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57b9e627",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000644 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2053\n",
      "[LightGBM] [Info] Number of data points in the train set: 2880, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 1.125000\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000516 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2053\n",
      "[LightGBM] [Info] Number of data points in the train set: 2880, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 1.138542\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000335 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2023\n",
      "[LightGBM] [Info] Number of data points in the train set: 2880, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 1.125000\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000296 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2023\n",
      "[LightGBM] [Info] Number of data points in the train set: 2880, number of used features: 41\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000279 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2023\n",
      "[LightGBM] [Info] Number of data points in the train set: 2880, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 3.000000\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000412 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2023\n",
      "[LightGBM] [Info] Number of data points in the train set: 2880, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 1.126736\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000383 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2023\n",
      "[LightGBM] [Info] Number of data points in the train set: 2880, number of used features: 41\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000340 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2023\n",
      "[LightGBM] [Info] Number of data points in the train set: 2880, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 3.000000\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000278 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2023\n",
      "[LightGBM] [Info] Number of data points in the train set: 2880, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 1.129514\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000335 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2023\n",
      "[LightGBM] [Info] Number of data points in the train set: 2880, number of used features: 41\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000331 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2023\n",
      "[LightGBM] [Info] Number of data points in the train set: 2880, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 3.000000\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000281 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2023\n",
      "[LightGBM] [Info] Number of data points in the train set: 2880, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 1.134722\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000419 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2023\n",
      "[LightGBM] [Info] Number of data points in the train set: 2880, number of used features: 41\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000345 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2023\n",
      "[LightGBM] [Info] Number of data points in the train set: 2880, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 3.000000\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000320 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2023\n",
      "[LightGBM] [Info] Number of data points in the train set: 2880, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 1.137500\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000379 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2023\n",
      "[LightGBM] [Info] Number of data points in the train set: 2880, number of used features: 41\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000298 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2023\n",
      "[LightGBM] [Info] Number of data points in the train set: 2880, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 3.000000\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000394 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2023\n",
      "[LightGBM] [Info] Number of data points in the train set: 2880, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 1.138542\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000433 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2023\n",
      "[LightGBM] [Info] Number of data points in the train set: 2880, number of used features: 41\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000351 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2023\n",
      "[LightGBM] [Info] Number of data points in the train set: 2880, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 3.000000\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000440 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2023\n",
      "[LightGBM] [Info] Number of data points in the train set: 2880, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 1.138542\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000344 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2023\n",
      "[LightGBM] [Info] Number of data points in the train set: 2880, number of used features: 41\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000385 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2023\n",
      "[LightGBM] [Info] Number of data points in the train set: 2880, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 3.000000\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000485 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2079\n",
      "[LightGBM] [Info] Number of data points in the train set: 2936, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 1.138283\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000488 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2079\n",
      "[LightGBM] [Info] Number of data points in the train set: 2936, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 1.151567\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000284 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2050\n",
      "[LightGBM] [Info] Number of data points in the train set: 2936, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 1.138283\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000382 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2050\n",
      "[LightGBM] [Info] Number of data points in the train set: 2936, number of used features: 41\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000338 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2050\n",
      "[LightGBM] [Info] Number of data points in the train set: 2936, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 3.000000\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000297 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2050\n",
      "[LightGBM] [Info] Number of data points in the train set: 2936, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 1.141349\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000335 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2050\n",
      "[LightGBM] [Info] Number of data points in the train set: 2936, number of used features: 41\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000344 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2050\n",
      "[LightGBM] [Info] Number of data points in the train set: 2936, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 3.000000\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000295 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2050\n",
      "[LightGBM] [Info] Number of data points in the train set: 2936, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 1.144414\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000293 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2050\n",
      "[LightGBM] [Info] Number of data points in the train set: 2936, number of used features: 41\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000334 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2050\n",
      "[LightGBM] [Info] Number of data points in the train set: 2936, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 3.000000\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000367 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2050\n",
      "[LightGBM] [Info] Number of data points in the train set: 2936, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 1.147139\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000347 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2050\n",
      "[LightGBM] [Info] Number of data points in the train set: 2936, number of used features: 41\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000281 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2050\n",
      "[LightGBM] [Info] Number of data points in the train set: 2936, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 3.000000\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000357 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2050\n",
      "[LightGBM] [Info] Number of data points in the train set: 2936, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 1.149864\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000309 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2050\n",
      "[LightGBM] [Info] Number of data points in the train set: 2936, number of used features: 41\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000299 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2050\n",
      "[LightGBM] [Info] Number of data points in the train set: 2936, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 3.000000\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000292 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2050\n",
      "[LightGBM] [Info] Number of data points in the train set: 2936, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 1.150886\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000286 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2050\n",
      "[LightGBM] [Info] Number of data points in the train set: 2936, number of used features: 41\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000299 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2050\n",
      "[LightGBM] [Info] Number of data points in the train set: 2936, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 3.000000\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000295 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2050\n",
      "[LightGBM] [Info] Number of data points in the train set: 2936, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 1.151567\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000302 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2050\n",
      "[LightGBM] [Info] Number of data points in the train set: 2936, number of used features: 41\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000355 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2050\n",
      "[LightGBM] [Info] Number of data points in the train set: 2936, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 3.000000\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000509 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2085\n",
      "[LightGBM] [Info] Number of data points in the train set: 2992, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 1.149398\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000643 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2085\n",
      "[LightGBM] [Info] Number of data points in the train set: 2992, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 1.161430\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000431 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2056\n",
      "[LightGBM] [Info] Number of data points in the train set: 2992, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 1.149398\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000478 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2056\n",
      "[LightGBM] [Info] Number of data points in the train set: 2992, number of used features: 41\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000393 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2056\n",
      "[LightGBM] [Info] Number of data points in the train set: 2992, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 3.000000\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000346 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2056\n",
      "[LightGBM] [Info] Number of data points in the train set: 2992, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 1.151404\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000296 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2056\n",
      "[LightGBM] [Info] Number of data points in the train set: 2992, number of used features: 41\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000298 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2056\n",
      "[LightGBM] [Info] Number of data points in the train set: 2992, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 3.000000\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000386 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2056\n",
      "[LightGBM] [Info] Number of data points in the train set: 2992, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 1.153743\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000299 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2056\n",
      "[LightGBM] [Info] Number of data points in the train set: 2992, number of used features: 41\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000293 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2056\n",
      "[LightGBM] [Info] Number of data points in the train set: 2992, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 3.000000\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000339 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2056\n",
      "[LightGBM] [Info] Number of data points in the train set: 2992, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 1.156417\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000393 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2056\n",
      "[LightGBM] [Info] Number of data points in the train set: 2992, number of used features: 41\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000288 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2056\n",
      "[LightGBM] [Info] Number of data points in the train set: 2992, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 3.000000\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000350 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2056\n",
      "[LightGBM] [Info] Number of data points in the train set: 2992, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 1.161096\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000319 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2056\n",
      "[LightGBM] [Info] Number of data points in the train set: 2992, number of used features: 41\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000336 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2056\n",
      "[LightGBM] [Info] Number of data points in the train set: 2992, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 3.000000\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000293 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2056\n",
      "[LightGBM] [Info] Number of data points in the train set: 2992, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 1.161765\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000356 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2056\n",
      "[LightGBM] [Info] Number of data points in the train set: 2992, number of used features: 41\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000428 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2056\n",
      "[LightGBM] [Info] Number of data points in the train set: 2992, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 3.000000\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000395 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2056\n",
      "[LightGBM] [Info] Number of data points in the train set: 2992, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 1.161430\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000461 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2056\n",
      "[LightGBM] [Info] Number of data points in the train set: 2992, number of used features: 41\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000395 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2056\n",
      "[LightGBM] [Info] Number of data points in the train set: 2992, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 3.000000\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000490 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2093\n",
      "[LightGBM] [Info] Number of data points in the train set: 3048, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 1.157808\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000507 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2093\n",
      "[LightGBM] [Info] Number of data points in the train set: 3048, number of used features: 49\n",
      "[LightGBM] [Info] Start training from score 1.175853\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000406 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2064\n",
      "[LightGBM] [Info] Number of data points in the train set: 3048, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 1.157808\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000392 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2064\n",
      "[LightGBM] [Info] Number of data points in the train set: 3048, number of used features: 41\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000380 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2064\n",
      "[LightGBM] [Info] Number of data points in the train set: 3048, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 3.000000\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000296 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2064\n",
      "[LightGBM] [Info] Number of data points in the train set: 3048, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 1.161089\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000289 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2064\n",
      "[LightGBM] [Info] Number of data points in the train set: 3048, number of used features: 41\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000387 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2064\n",
      "[LightGBM] [Info] Number of data points in the train set: 3048, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 3.000000\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000333 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2064\n",
      "[LightGBM] [Info] Number of data points in the train set: 3048, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 1.164698\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000427 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2064\n",
      "[LightGBM] [Info] Number of data points in the train set: 3048, number of used features: 41\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000404 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2064\n",
      "[LightGBM] [Info] Number of data points in the train set: 3048, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 3.000000\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000414 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2064\n",
      "[LightGBM] [Info] Number of data points in the train set: 3048, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 1.169948\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000371 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2064\n",
      "[LightGBM] [Info] Number of data points in the train set: 3048, number of used features: 41\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000483 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2064\n",
      "[LightGBM] [Info] Number of data points in the train set: 3048, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 3.000000\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000335 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2064\n",
      "[LightGBM] [Info] Number of data points in the train set: 3048, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 1.173556\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000293 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2064\n",
      "[LightGBM] [Info] Number of data points in the train set: 3048, number of used features: 41\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000295 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2064\n",
      "[LightGBM] [Info] Number of data points in the train set: 3048, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 3.000000\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000386 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2064\n",
      "[LightGBM] [Info] Number of data points in the train set: 3048, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 1.175853\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000395 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2064\n",
      "[LightGBM] [Info] Number of data points in the train set: 3048, number of used features: 41\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000352 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2064\n",
      "[LightGBM] [Info] Number of data points in the train set: 3048, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 3.000000\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000338 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2064\n",
      "[LightGBM] [Info] Number of data points in the train set: 3048, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 1.175853\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000284 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2064\n",
      "[LightGBM] [Info] Number of data points in the train set: 3048, number of used features: 41\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000323 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2064\n",
      "[LightGBM] [Info] Number of data points in the train set: 3048, number of used features: 41\n",
      "[LightGBM] [Info] Start training from score 3.000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>sMAPE</th>\n",
       "      <th>COV_p10_p90_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.081485</td>\n",
       "      <td>1.476333</td>\n",
       "      <td>41.557781</td>\n",
       "      <td>87.718469</td>\n",
       "      <td>68.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.086805</td>\n",
       "      <td>1.580462</td>\n",
       "      <td>46.019083</td>\n",
       "      <td>100.198492</td>\n",
       "      <td>65.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.134352</td>\n",
       "      <td>1.517946</td>\n",
       "      <td>51.032503</td>\n",
       "      <td>83.396717</td>\n",
       "      <td>62.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.018458</td>\n",
       "      <td>1.286023</td>\n",
       "      <td>73.645991</td>\n",
       "      <td>82.796613</td>\n",
       "      <td>68.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.218990</td>\n",
       "      <td>1.709245</td>\n",
       "      <td>51.703950</td>\n",
       "      <td>95.033108</td>\n",
       "      <td>56.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.263186</td>\n",
       "      <td>1.732761</td>\n",
       "      <td>75.913090</td>\n",
       "      <td>123.179930</td>\n",
       "      <td>62.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.658472</td>\n",
       "      <td>0.793765</td>\n",
       "      <td>56.572403</td>\n",
       "      <td>114.014686</td>\n",
       "      <td>71.875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        MAE      RMSE       MAPE       sMAPE  COV_p10_p90_%\n",
       "1  1.081485  1.476333  41.557781   87.718469         68.750\n",
       "2  1.086805  1.580462  46.019083  100.198492         65.625\n",
       "3  1.134352  1.517946  51.032503   83.396717         62.500\n",
       "4  1.018458  1.286023  73.645991   82.796613         68.750\n",
       "5  1.218990  1.709245  51.703950   95.033108         56.250\n",
       "6  1.263186  1.732761  75.913090  123.179930         62.500\n",
       "7  0.658472  0.793765  56.572403  114.014686         71.875"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>sMAPE</th>\n",
       "      <th>COV_p10_p90_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.065964</td>\n",
       "      <td>1.473067</td>\n",
       "      <td>55.987553</td>\n",
       "      <td>98.048288</td>\n",
       "      <td>65.178571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        MAE      RMSE       MAPE      sMAPE  COV_p10_p90_%\n",
       "0  1.065964  1.473067  55.987553  98.048288      65.178571"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mlflow] warning leyendo overall 'results\\lgbm_direct_metrics_overall.csv': Invalid metric name: 'COV_p10_p90_%'. Names may only contain alphanumerics, underscores (_), dashes (-), periods (.), spaces ( ), and slashes (/).\n",
      "[mlflow] warning leyendo by_h 'results\\lgbm_direct_metrics_by_h.csv': Invalid metric name: 'COV_p10_p90_%_by_h'. Names may only contain alphanumerics, underscores (_), dashes (-), periods (.), spaces ( ), and slashes (/).\n"
     ]
    }
   ],
   "source": [
    "#18\n",
    "lgbm_all = []\n",
    "splits = rolling_origins(df_l[\"date\"], n_origins=N_ORIGINS, horizon=HORIZON)\n",
    "\n",
    "for (train_end, test_start, test_end) in splits:\n",
    "    train = df_l[df_l[\"date\"] <= train_end].copy()\n",
    "    test  = df_l[(df_l[\"date\"] >= test_start) & (df_l[\"date\"] <= test_end)].copy()\n",
    "\n",
    "    if train[\"date\"].nunique() < MIN_TRAIN_DAYS:\n",
    "        print(f\"[LGBM] Split saltado: historia insuficiente ({train['date'].nunique()} días).\")\n",
    "        continue\n",
    "\n",
    "    # Importancias por split (anti-snooping)\n",
    "    imp_h1 = lgbm_importance_until(train, candidates, \"y_1\", train_end).head(TOPK_IMP)\n",
    "    imp_h7 = lgbm_importance_until(train, candidates, \"y_7\", train_end).head(TOPK_IMP)\n",
    "    selected_feats = sorted(set(imp_h1[\"feature\"]).union(set(imp_h7[\"feature\"])))\n",
    "\n",
    "    preds_blocks = []\n",
    "    for h in range(1, HORIZON + 1):\n",
    "        y_col = f\"y_{h}\"\n",
    "        tr = train.dropna(subset=[y_col]).copy()\n",
    "        if tr.empty:\n",
    "            continue\n",
    "\n",
    "        X_tr, y_tr = tr[selected_feats], tr[y_col]\n",
    "        if USE_LOG1P_TARGET:\n",
    "            y_tr = np.log1p(y_tr)\n",
    "\n",
    "        # Ciclo pequeño de objetivos: auto vs poisson/tweedie (se elige el mejor en train por MAE)\n",
    "        candidate_models = []\n",
    "        objectives_to_try = [\"auto\"]\n",
    "        if LGBM_OBJECTIVE in [\"poisson\", \"tweedie\"]:\n",
    "            objectives_to_try = [LGBM_OBJECTIVE]\n",
    "        if \"tweedie\" in objectives_to_try:\n",
    "            tweedie_grid = TWEEDIE_POWERS\n",
    "        else:\n",
    "            tweedie_grid = [None]\n",
    "\n",
    "        best_model = None\n",
    "        best_mae = np.inf\n",
    "        for obj in objectives_to_try:\n",
    "            for power in tweedie_grid:\n",
    "                mtmp = _fit_lgbm(X_tr, y_tr, objective=obj, tweedie_power=power)\n",
    "                # uso in-sample MAE como heurística rápida (alternativa: CV por fecha)\n",
    "                y_pred_tr = mtmp.predict(X_tr)\n",
    "                if USE_LOG1P_TARGET:\n",
    "                    y_pred_tr = np.expm1(y_pred_tr)\n",
    "                cur_mae = mae(tr[y_col], y_pred_tr)\n",
    "                if cur_mae < best_mae:\n",
    "                    best_mae = cur_mae\n",
    "                    best_model = (mtmp, obj, power)\n",
    "\n",
    "        model_c, used_obj, used_power = best_model\n",
    "\n",
    "        model_p10 = _fit_lgbm_quantile(X_tr, y_tr, alpha=0.10)\n",
    "        model_p90 = _fit_lgbm_quantile(X_tr, y_tr, alpha=0.90)\n",
    "\n",
    "        test_block = test.copy()\n",
    "        test_block[\"h\"] = (test_block[\"date\"] - test_start).dt.days + 1\n",
    "        mask_h = test_block[\"h\"] == h\n",
    "        X_te = test_block.loc[mask_h, selected_feats]\n",
    "\n",
    "        yhat_c   = model_c.predict(X_te)\n",
    "        yhat_p10 = model_p10.predict(X_te)\n",
    "        yhat_p90 = model_p90.predict(X_te)\n",
    "\n",
    "        if USE_LOG1P_TARGET:\n",
    "            yhat_c   = np.expm1(yhat_c)\n",
    "            yhat_p10 = np.expm1(yhat_p10)\n",
    "            yhat_p90 = np.expm1(yhat_p90)\n",
    "\n",
    "        out = test_block.loc[mask_h, [\"date\", \"product\"]].copy()\n",
    "        out[\"h\"] = h\n",
    "        out[\"yhat\"] = yhat_c\n",
    "        out[\"yhat_p10\"] = yhat_p10\n",
    "        out[\"yhat_p90\"] = yhat_p90\n",
    "        out[\"lgbm_objective\"] = used_obj\n",
    "        out[\"tweedie_power\"] = used_power\n",
    "        preds_blocks.append(out)\n",
    "\n",
    "    if preds_blocks:\n",
    "        fold_preds = pd.concat(preds_blocks, ignore_index=True)\n",
    "        y_true = test[[\"date\", \"product\", TARGET]].copy()\n",
    "        merged = y_true.merge(fold_preds, on=[\"date\", \"product\"], how=\"left\")\n",
    "        merged[\"model\"] = \"lgbm_direct\"\n",
    "        lgbm_all.append(merged)\n",
    "\n",
    "lgbm_results = pd.concat(lgbm_all, ignore_index=True) if lgbm_all else pd.DataFrame(columns=[\"date\",\"product\",TARGET,\"h\",\"yhat\",\"yhat_p10\",\"yhat_p90\",\"model\"])\n",
    "by_h_lgbm, overall_lgbm = summarize_metrics(lgbm_results.rename(columns={TARGET: \"y\"}))\n",
    "display(by_h_lgbm)\n",
    "display(pd.DataFrame([overall_lgbm]))\n",
    "# Estandarizar by_h: asegurar columna 'h'\n",
    "if \"h\" not in by_h_lgbm.columns:\n",
    "    by_h_lgbm = by_h_lgbm.reset_index()\n",
    "    if \"index\" in by_h_lgbm.columns and \"h\" not in by_h_lgbm.columns:\n",
    "        by_h_lgbm = by_h_lgbm.rename(columns={\"index\": \"h\"})\n",
    "by_h_lgbm = by_h_lgbm.sort_values(\"h\").reset_index(drop=True)\n",
    "\n",
    "RESULTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "lgbm_results.to_csv(RESULTS_DIR / \"lgbm_direct_forecasts.csv\", index=False)\n",
    "lgbm_results_norm = _normalize_for_forecast_save(lgbm_results, TARGET)\n",
    "lgbm_results_norm.to_csv(RESULTS_DIR / \"lgbm_direct_forecasts.csv\", index=False)\n",
    "by_h_lgbm.to_csv(RESULTS_DIR / \"lgbm_direct_metrics_by_h.csv\", index= False)\n",
    "pd.DataFrame([overall_lgbm]).to_csv(RESULTS_DIR / \"lgbm_direct_metrics_overall.csv\", index=False)\n",
    "\n",
    "\n",
    "_mlflow_log_run(\n",
    "    model_name=\"lgbm_direct\",\n",
    "    params={\n",
    "        \"HORIZON\": HORIZON,\n",
    "        \"N_ORIGINS\": N_ORIGINS,\n",
    "        \"USE_LOG1P_TARGET\": USE_LOG1P_TARGET,\n",
    "        \"topK_feats\": \"auto/según tu código\"\n",
    "    },\n",
    "    overall_csv=RESULTS_DIR / \"lgbm_direct_metrics_overall.csv\",\n",
    "    by_h_csv=RESULTS_DIR / \"lgbm_direct_metrics_by_h.csv\",\n",
    "    extra_artifacts=[\n",
    "        RESULTS_DIR / \"lgbm_direct_forecasts.csv\",\n",
    "        RESULTS_DIR / \"lgbm_direct_importances.csv\",  \n",
    "    ],\n",
    "    tags={\"family\": \"lgbm\", \"stage\": \"train\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf271ea9-ec7a-4f96-8aba-1b628b63364c",
   "metadata": {},
   "source": [
    "## PROPHET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b5d76de-2cd6-45be-956e-f6e21f6c9a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n",
      "04:08:56 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Prophet] Regresores detectados: ['is_holiday', 'wx_temperature_2m', 'wx_precipitation', 'wx_cloudcover', 'is_holiday_prev', 'is_holiday_next']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:09:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "04:09:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "04:09:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "04:09:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "04:09:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "04:09:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "04:09:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "04:09:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "04:09:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "04:09:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "04:09:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "04:09:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "04:09:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "04:09:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "04:09:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "04:09:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "04:09:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "04:09:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "04:09:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "04:09:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "04:09:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "04:09:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "04:09:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "04:09:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "04:09:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "04:09:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "04:09:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "04:09:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "04:09:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "04:09:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "04:09:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "04:09:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "04:09:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "04:09:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "04:09:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "04:09:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "04:09:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "04:09:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "04:09:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "04:09:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "04:09:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "04:09:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "04:09:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "04:09:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "04:09:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "04:09:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "04:09:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "04:09:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "04:09:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "04:09:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "04:09:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "04:09:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "04:09:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "04:09:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "04:09:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "04:09:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "04:09:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "04:09:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "04:09:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "04:09:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "04:09:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "04:09:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "04:09:05 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>sMAPE</th>\n",
       "      <th>COV_p10_p90_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.293938</td>\n",
       "      <td>1.815439</td>\n",
       "      <td>49.425003</td>\n",
       "      <td>97.644091</td>\n",
       "      <td>71.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.155643</td>\n",
       "      <td>1.642031</td>\n",
       "      <td>52.707088</td>\n",
       "      <td>100.350352</td>\n",
       "      <td>81.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.228788</td>\n",
       "      <td>1.591474</td>\n",
       "      <td>49.030049</td>\n",
       "      <td>78.305114</td>\n",
       "      <td>75.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.998436</td>\n",
       "      <td>1.399174</td>\n",
       "      <td>44.401634</td>\n",
       "      <td>78.734236</td>\n",
       "      <td>81.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.518304</td>\n",
       "      <td>2.157232</td>\n",
       "      <td>51.272682</td>\n",
       "      <td>101.870488</td>\n",
       "      <td>71.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.969899</td>\n",
       "      <td>1.203281</td>\n",
       "      <td>59.714583</td>\n",
       "      <td>127.236567</td>\n",
       "      <td>84.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.715869</td>\n",
       "      <td>0.938018</td>\n",
       "      <td>52.070689</td>\n",
       "      <td>109.388430</td>\n",
       "      <td>90.625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        MAE      RMSE       MAPE       sMAPE  COV_p10_p90_%\n",
       "1  1.293938  1.815439  49.425003   97.644091         71.875\n",
       "2  1.155643  1.642031  52.707088  100.350352         81.250\n",
       "3  1.228788  1.591474  49.030049   78.305114         75.000\n",
       "4  0.998436  1.399174  44.401634   78.734236         81.250\n",
       "5  1.518304  2.157232  51.272682  101.870488         71.875\n",
       "6  0.969899  1.203281  59.714583  127.236567         84.375\n",
       "7  0.715869  0.938018  52.070689  109.388430         90.625"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>sMAPE</th>\n",
       "      <th>COV_p10_p90_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.12584</td>\n",
       "      <td>1.579511</td>\n",
       "      <td>50.612771</td>\n",
       "      <td>99.075611</td>\n",
       "      <td>79.464286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       MAE      RMSE       MAPE      sMAPE  COV_p10_p90_%\n",
       "0  1.12584  1.579511  50.612771  99.075611      79.464286"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mlflow] warning leyendo overall 'results\\prophet_metrics_overall.csv': Invalid metric name: 'COV_p10_p90_%'. Names may only contain alphanumerics, underscores (_), dashes (-), periods (.), spaces ( ), and slashes (/).\n",
      "[mlflow] warning leyendo by_h 'results\\prophet_metrics_by_h.csv': Invalid metric name: 'COV_p10_p90_%_by_h'. Names may only contain alphanumerics, underscores (_), dashes (-), periods (.), spaces ( ), and slashes (/).\n"
     ]
    }
   ],
   "source": [
    "#19\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "    PROPHET_OK = True\n",
    "except Exception as e:\n",
    "    print(\"Prophet no disponible, se omitirá. Error:\", e)\n",
    "    PROPHET_OK = False\n",
    "\n",
    "prophet_all = []\n",
    "\n",
    "candidate_regressors = [c for c in [\"is_holiday\",\"wx_temperature_2m\",\"wx_precipitation\",\"wx_cloudcover\",\n",
    "                                    \"is_holiday_ext\",\"is_holiday_prev\",\"is_holiday_next\"] if c in df.columns]\n",
    "print(\"[Prophet] Regresores detectados:\", candidate_regressors)\n",
    "\n",
    "if PROPHET_OK:\n",
    "    splits = rolling_origins(df[\"date\"], n_origins=N_ORIGINS, horizon=HORIZON)\n",
    "    for (train_end, test_start, test_end) in splits:\n",
    "        train = df[df[\"date\"] <= train_end].copy()\n",
    "        test  = df[(df[\"date\"] >= test_start) & (df[\"date\"] <= test_end)].copy()\n",
    "        if train[\"date\"].nunique() < MIN_TRAIN_DAYS:\n",
    "            print(f\"[Prophet] Split saltado por historia insuficiente ({train['date'].nunique()} días).\")\n",
    "            continue\n",
    "\n",
    "        fold = []\n",
    "        for prod, g in train.groupby(\"product\"):\n",
    "            g = g.sort_values(\"date\")\n",
    "            aux = g.rename(columns={\"date\":\"ds\", TARGET:\"y\"}).copy()\n",
    "\n",
    "            m = Prophet(interval_width=0.80, weekly_seasonality=True,\n",
    "                        daily_seasonality=False, yearly_seasonality=False)\n",
    "\n",
    "            if PROPHET_USE_REGRESSORS and candidate_regressors:\n",
    "                for reg in candidate_regressors:\n",
    "                    m.add_regressor(reg)\n",
    "\n",
    "            cols_fit = [\"ds\",\"y\"] + (candidate_regressors if PROPHET_USE_REGRESSORS else [])\n",
    "            aux[cols_fit] = aux[cols_fit].sort_values(\"ds\").ffill().bfill()\n",
    "            m.fit(aux[cols_fit])\n",
    "\n",
    "            future = pd.DataFrame({\"ds\": pd.date_range(test_start, test_end, freq=\"D\")})\n",
    "            if PROPHET_USE_REGRESSORS and candidate_regressors:\n",
    "                regs_future = (df[df[\"product\"]==prod]\n",
    "                               .rename(columns={\"date\":\"ds\"})[[\"ds\"] + candidate_regressors]\n",
    "                               .drop_duplicates(subset=[\"ds\"])\n",
    "                               .sort_values(\"ds\")\n",
    "                               .ffill().bfill())\n",
    "                future = future.merge(regs_future, on=\"ds\", how=\"left\")\n",
    "                future[candidate_regressors] = future[candidate_regressors].ffill().bfill()\n",
    "\n",
    "            # >>> CAMBIO CLAVE: traer bandas y renombrar a estándar\n",
    "            fcst = m.predict(future)[[\"ds\",\"yhat\",\"yhat_lower\",\"yhat_upper\"]]\n",
    "            fcst = fcst.rename(columns={\"yhat_lower\":\"yhat_p10\", \"yhat_upper\":\"yhat_p90\"})  # <- bandas p10/p90\n",
    "            fcst[\"product\"] = prod\n",
    "            fold.append(fcst.rename(columns={\"ds\":\"date\"}))\n",
    "            # <<<\n",
    "\n",
    "        fold = pd.concat(fold, ignore_index=True) if fold else pd.DataFrame(columns=[\"date\",\"yhat\",\"yhat_p10\",\"yhat_p90\",\"product\"])\n",
    "        mask = (fold[\"date\"] >= test_start) & (fold[\"date\"] <= test_end)\n",
    "        fold = fold.loc[mask].copy()\n",
    "        fold[\"h\"] = (fold[\"date\"] - test_start).dt.days + 1\n",
    "\n",
    "        y_true = test[[\"date\",\"product\",TARGET]].copy()\n",
    "        merged = y_true.merge(fold, on=[\"date\",\"product\"], how=\"left\")\n",
    "        merged[\"model\"] = \"prophet\"\n",
    "        prophet_all.append(merged)\n",
    "\n",
    "    # Resultados del experimento Prophet\n",
    "    prophet_results = (pd.concat(prophet_all, ignore_index=True)\n",
    "                       if prophet_all else\n",
    "                       pd.DataFrame(columns=[\"date\",\"product\",TARGET,\"h\",\"yhat\",\"yhat_p10\",\"yhat_p90\",\"model\"]))\n",
    "\n",
    "    # Por compatibilidad si llegaran a venir columnas originales:\n",
    "    if \"yhat_lower\" in prophet_results.columns and \"yhat_p10\" not in prophet_results.columns:\n",
    "        prophet_results[\"yhat_p10\"] = prophet_results[\"yhat_lower\"].astype(float)\n",
    "    if \"yhat_upper\" in prophet_results.columns and \"yhat_p90\" not in prophet_results.columns:\n",
    "        prophet_results[\"yhat_p90\"] = prophet_results[\"yhat_upper\"].astype(float)\n",
    "\n",
    "    # Asegurar 'h' visible si quedó como índice en algún paso\n",
    "    if \"h\" not in prophet_results.columns:\n",
    "        prophet_results = prophet_results.reset_index()\n",
    "        if \"index\" in prophet_results.columns and \"h\" not in prophet_results.columns:\n",
    "            prophet_results = prophet_results.rename(columns={\"index\": \"h\"})\n",
    "\n",
    "    # Métricas (usa cobertura si yhat_p10/p90 existen)\n",
    "    by_h_prophet, overall_prophet = summarize_metrics(\n",
    "        prophet_results.rename(columns={TARGET: \"y\"})\n",
    "    )\n",
    "    display(by_h_prophet)\n",
    "    display(pd.DataFrame([overall_prophet]))\n",
    "\n",
    "    # Guardados: by_h CON columna 'h' y sin índice\n",
    "    if \"h\" not in by_h_prophet.columns:\n",
    "        by_h_prophet = by_h_prophet.reset_index()\n",
    "        if \"index\" in by_h_prophet.columns and \"h\" not in by_h_prophet.columns:\n",
    "            by_h_prophet = by_h_prophet.rename(columns={\"index\": \"h\"})\n",
    "    by_h_prophet = by_h_prophet.sort_values(\"h\").reset_index(drop=True)\n",
    "\n",
    "    RESULTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "    prophet_results.to_csv(RESULTS_DIR / \"prophet_forecasts.csv\", index=False)\n",
    "    prophet_results_norm = _normalize_for_forecast_save(prophet_results, TARGET)\n",
    "    prophet_results_norm.to_csv(RESULTS_DIR / \"prophet_forecasts.csv\", index=False)\n",
    "    by_h_prophet.to_csv(RESULTS_DIR / \"prophet_metrics_by_h.csv\", index=False)\n",
    "    pd.DataFrame([overall_prophet]).to_csv(RESULTS_DIR / \"prophet_metrics_overall.csv\", index=False)\n",
    "else:\n",
    "    print(\"Saltando Prophet.\")\n",
    "\n",
    "\n",
    "_mlflow_log_run(\n",
    "    model_name=\"prophet\",\n",
    "    params={\"HORIZON\": HORIZON, \"use_regressors\": bool(PROPHET_USE_REGRESSORS)},\n",
    "    overall_csv=RESULTS_DIR / \"prophet_metrics_overall.csv\",\n",
    "    by_h_csv=RESULTS_DIR / \"prophet_metrics_by_h.csv\",\n",
    "    extra_artifacts=[RESULTS_DIR / \"prophet_forecasts.csv\"],\n",
    "    tags={\"family\": \"prophet\", \"stage\": \"train\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fbf46e-ef85-493d-8adb-0dfe72ac2516",
   "metadata": {},
   "source": [
    "## SARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2816a2c3-64e1-489d-a883-e498ec37f919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SARIMAX] exog_cols: ['is_holiday', 'is_holiday_prev', 'is_holiday_next', 'wx_temperature_2m', 'wx_precipitation', 'wx_cloudcover']\n",
      "[SARIMAX] Split 2025-02-23 | train_days=360 | test_days=7\n",
      "[SARIMAX] Split 2025-03-02 | train_days=367 | test_days=7\n",
      "[SARIMAX] Split 2025-03-09 | train_days=374 | test_days=7\n",
      "[SARIMAX] Split 2025-03-16 | train_days=381 | test_days=7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Julian\\miniconda3\\envs\\coffee310\\lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>sMAPE</th>\n",
       "      <th>COV_p10_p90_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.239872</td>\n",
       "      <td>1.851112</td>\n",
       "      <td>48.132830</td>\n",
       "      <td>97.270426</td>\n",
       "      <td>68.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.905599</td>\n",
       "      <td>1.275229</td>\n",
       "      <td>44.474806</td>\n",
       "      <td>95.156047</td>\n",
       "      <td>87.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.933366</td>\n",
       "      <td>1.234957</td>\n",
       "      <td>43.512605</td>\n",
       "      <td>72.314918</td>\n",
       "      <td>84.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.052289</td>\n",
       "      <td>1.419910</td>\n",
       "      <td>54.125328</td>\n",
       "      <td>86.942540</td>\n",
       "      <td>81.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.236522</td>\n",
       "      <td>1.812889</td>\n",
       "      <td>47.647011</td>\n",
       "      <td>95.928120</td>\n",
       "      <td>75.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.900183</td>\n",
       "      <td>1.225605</td>\n",
       "      <td>66.324917</td>\n",
       "      <td>126.536236</td>\n",
       "      <td>90.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.878168</td>\n",
       "      <td>1.172474</td>\n",
       "      <td>76.247366</td>\n",
       "      <td>116.867289</td>\n",
       "      <td>84.375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        MAE      RMSE       MAPE       sMAPE  COV_p10_p90_%\n",
       "1  1.239872  1.851112  48.132830   97.270426         68.750\n",
       "2  0.905599  1.275229  44.474806   95.156047         87.500\n",
       "3  0.933366  1.234957  43.512605   72.314918         84.375\n",
       "4  1.052289  1.419910  54.125328   86.942540         81.250\n",
       "5  1.236522  1.812889  47.647011   95.928120         75.000\n",
       "6  0.900183  1.225605  66.324917  126.536236         90.625\n",
       "7  0.878168  1.172474  76.247366  116.867289         84.375"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>sMAPE</th>\n",
       "      <th>COV_p10_p90_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.020857</td>\n",
       "      <td>1.451967</td>\n",
       "      <td>52.824646</td>\n",
       "      <td>98.716511</td>\n",
       "      <td>81.696429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        MAE      RMSE       MAPE      sMAPE  COV_p10_p90_%\n",
       "0  1.020857  1.451967  52.824646  98.716511      81.696429"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mlflow] warning leyendo overall 'results\\sarimax_metrics_overall.csv': Invalid metric name: 'COV_p10_p90_%'. Names may only contain alphanumerics, underscores (_), dashes (-), periods (.), spaces ( ), and slashes (/).\n",
      "[mlflow] warning leyendo by_h 'results\\sarimax_metrics_by_h.csv': Invalid metric name: 'COV_p10_p90_%_by_h'. Names may only contain alphanumerics, underscores (_), dashes (-), periods (.), spaces ( ), and slashes (/).\n"
     ]
    }
   ],
   "source": [
    "#20\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "    SARIMAX_OK = True\n",
    "except Exception as e:\n",
    "    print(\"statsmodels no disponible para SARIMAX. Error:\", e)\n",
    "    SARIMAX_OK = False\n",
    "\n",
    "sarimax_all = []\n",
    "if SARIMAX_OK:\n",
    "    splits = rolling_origins(df[\"date\"], n_origins=N_ORIGINS, horizon=HORIZON)\n",
    "    exog_cols = [c for c in [\"is_holiday\",\"is_holiday_prev\",\"is_holiday_next\",\n",
    "                             \"wx_temperature_2m\",\"wx_precipitation\",\"wx_cloudcover\"] if c in df.columns]\n",
    "    print(f\"[SARIMAX] exog_cols: {exog_cols}\")\n",
    "\n",
    "    for (train_end, test_start, test_end) in splits:\n",
    "        train = df[df[\"date\"] <= train_end].copy()\n",
    "        test  = df[(df[\"date\"] >= test_start) & (df[\"date\"] <= test_end)].copy()\n",
    "        n_days = train[\"date\"].nunique()\n",
    "        print(f\"[SARIMAX] Split {train_end.date()} | train_days={n_days} | test_days={test['date'].nunique()}\")\n",
    "        if n_days < MIN_TRAIN_DAYS:\n",
    "            print(f\"[SARIMAX] Split saltado por historia insuficiente ({n_days} días).\")\n",
    "            continue\n",
    "\n",
    "        fold_parts = []\n",
    "        for prod, g in train.groupby(\"product\"):\n",
    "            g = g.sort_values(\"date\").set_index(\"date\")\n",
    "\n",
    "            # Target train (sin NaNs, frecuencia diaria)\n",
    "            y_tr = g[TARGET].astype(float).asfreq(\"D\").ffill().bfill()\n",
    "\n",
    "            # Exógenas train alineadas EXACTAS al índice del target\n",
    "            ex_tr = None\n",
    "            if exog_cols:\n",
    "                ex_tr = g[exog_cols].asfreq(\"D\").ffill().bfill()\n",
    "                ex_tr = ex_tr.reindex(y_tr.index)\n",
    "\n",
    "            # Ensayo de órdenes (simple y robusto)\n",
    "            orders_to_try = [((1,0,1),(1,0,1,7)), ((0,1,1),(0,1,1,7))]\n",
    "            res = None\n",
    "            for order, seasonal_order in orders_to_try:\n",
    "                try:\n",
    "                    model = sm.tsa.statespace.SARIMAX(\n",
    "                        y_tr, order=order, seasonal_order=seasonal_order,\n",
    "                        exog=ex_tr, enforce_stationarity=False, enforce_invertibility=False\n",
    "                    )\n",
    "                    res = model.fit(disp=False)\n",
    "                    chosen = (order, seasonal_order, True)  # con exog\n",
    "                    break\n",
    "                except Exception as e1:\n",
    "                    # Fallback sin exógenas\n",
    "                    try:\n",
    "                        model = sm.tsa.statespace.SARIMAX(\n",
    "                            y_tr, order=order, seasonal_order=seasonal_order,\n",
    "                            exog=None, enforce_stationarity=False, enforce_invertibility=False\n",
    "                        )\n",
    "                        res = model.fit(disp=False)\n",
    "                        ex_tr = None\n",
    "                        chosen = (order, seasonal_order, False)  # sin exog\n",
    "                        break\n",
    "                    except Exception as e2:\n",
    "                        continue\n",
    "\n",
    "            if res is None:\n",
    "                print(f\"[SARIMAX][{prod}] no se pudo ajustar en ningún orden (con/sin exog).\")\n",
    "                continue\n",
    "\n",
    "            # Exógenas test (alineadas a future_idx)\n",
    "            future_idx = pd.date_range(test_start, test_end, freq=\"D\")\n",
    "            ex_te = None\n",
    "            if exog_cols and chosen[2]:  # solo si el modelo fue con exog\n",
    "                g_full = df[df[\"product\"]==prod].set_index(\"date\")\n",
    "                ex_te = g_full[exog_cols].asfreq(\"D\").ffill().bfill().reindex(future_idx)\n",
    "\n",
    "            try:\n",
    "                fcst = res.get_forecast(steps=len(future_idx), exog=ex_te)\n",
    "                yhat = fcst.predicted_mean\n",
    "\n",
    "                #ntervalos p10–p90 para cobertura \n",
    "                conf = fcst.conf_int(alpha=0.20)  # 80% => p10-p90   \n",
    "                conf_cols = list(conf.columns)                         \n",
    "                # Detecta automáticamente columnas lower/upper        \n",
    "                if len(conf_cols) >= 2:                                \n",
    "                    lower_col = next((c for c in conf_cols if \"lower\" in c.lower()), conf_cols[0])\n",
    "                    upper_col = next((c for c in conf_cols if \"upper\" in c.lower()), conf_cols[1])\n",
    "                    yhat_p10 = conf[lower_col].values\n",
    "                    yhat_p90 = conf[upper_col].values\n",
    "                else:\n",
    "                    yhat_p10 = None\n",
    "                    yhat_p90 = None\n",
    "               \n",
    "            except Exception as e3:\n",
    "                print(f\"[SARIMAX][{prod}] fallo en forecast: {type(e3).__name__}: {e3}\")\n",
    "                continue\n",
    "\n",
    "            out = pd.DataFrame({\n",
    "                \"date\": future_idx,\n",
    "                \"product\": prod,\n",
    "                \"yhat\": yhat.values\n",
    "            })\n",
    "            \n",
    "            if yhat_p10 is not None and yhat_p90 is not None:       \n",
    "                out[\"yhat_p10\"] = yhat_p10                           \n",
    "                out[\"yhat_p90\"] = yhat_p90                           \n",
    "           \n",
    "\n",
    "            fold_parts.append(out)\n",
    "\n",
    "        if fold_parts:\n",
    "            fold = pd.concat(fold_parts, ignore_index=True)\n",
    "            fold[\"h\"] = (fold[\"date\"] - test_start).dt.days + 1\n",
    "            y_true = test[[\"date\",\"product\",TARGET]].copy()\n",
    "            merged = y_true.merge(fold, on=[\"date\",\"product\"], how=\"left\")\n",
    "            merged[\"model\"] = \"sarimax\"\n",
    "            sarimax_all.append(merged)\n",
    "        else:\n",
    "            print(f\"[SARIMAX] Split {train_end.date()} sin predicciones válidas (todos los productos fallaron).\")\n",
    "\n",
    "sarimax_results = (pd.concat(sarimax_all, ignore_index=True)\n",
    "                   if sarimax_all else\n",
    "                   pd.DataFrame(columns=[\"date\",\"product\",TARGET,\"h\",\"yhat\",\"yhat_p10\",\"yhat_p90\",\"model\"]))  \n",
    "\n",
    "if not sarimax_results.empty:\n",
    "    # Métricas (si yhat_p10/p90 existen, summarize_metrics calculará COV_p10_p90_%)\n",
    "    by_h_sarimax, overall_sarimax = summarize_metrics(sarimax_results.rename(columns={TARGET:\"y\"}))\n",
    "    display(by_h_sarimax)\n",
    "    display(pd.DataFrame([overall_sarimax]))\n",
    "\n",
    "    # Asegurar que 'h' esté como columna\n",
    "    if \"h\" not in by_h_sarimax.columns:\n",
    "        by_h_sarimax = by_h_sarimax.reset_index()\n",
    "        if \"index\" in by_h_sarimax.columns and \"h\" not in by_h_sarimax.columns:\n",
    "            by_h_sarimax = by_h_sarimax.rename(columns={\"index\":\"h\"})\n",
    "    by_h_sarimax = by_h_sarimax.sort_values(\"h\").reset_index(drop=True)\n",
    "\n",
    "    sarimax_results.to_csv(RESULTS_DIR / \"sarimax_forecasts.csv\", index=False)\n",
    "    sarimax_results_norm = _normalize_for_forecast_save(sarimax_results, TARGET)\n",
    "    sarimax_results_norm.to_csv(RESULTS_DIR / \"sarimax_forecasts.csv\", index=False)\n",
    "    by_h_sarimax.to_csv(RESULTS_DIR / \"sarimax_metrics_by_h.csv\", index=False) \n",
    "    pd.DataFrame([overall_sarimax]).to_csv(RESULTS_DIR / \"sarimax_metrics_overall.csv\", index=False)\n",
    "else:\n",
    "    print(\"Saltando resumen SARIMAX (sin resultados).\")\n",
    "\n",
    "\n",
    "_mlflow_log_run(\n",
    "    model_name=\"sarimax\",\n",
    "    params={\"HORIZON\": HORIZON, \"orders_searched\": \"[(1,0,1)x(1,0,1,7), (0,1,1)x(0,1,1,7)]\"},\n",
    "    overall_csv=RESULTS_DIR / \"sarimax_metrics_overall.csv\",\n",
    "    by_h_csv=RESULTS_DIR / \"sarimax_metrics_by_h.csv\",\n",
    "    extra_artifacts=[RESULTS_DIR / \"sarimax_forecasts.csv\"],\n",
    "    tags={\"family\": \"sarimax\", \"stage\": \"train\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b253082-4273-469c-a714-2a61d885b3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "''''# (reemplaza la celda de instalación anterior por esta)\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PKG_DIR = Path(\"./_keras_torch_pkgs\").resolve()  # <--- ANTES decía TARGET\n",
    "PKG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "!{sys.executable} -m pip install -q -t \"{PKG_DIR}\" \"keras==3.4.1\" \"torch==2.2.2\" \"numpy<2\"\n",
    "\n",
    "import sys, os\n",
    "sys.path.insert(0, str(PKG_DIR))\n",
    "os.environ.setdefault(\"KERAS_BACKEND\", \"torch\")\n",
    "\n",
    "import keras, torch\n",
    "from keras import layers\n",
    "print(\"Keras:\", keras.__version__, \"| Backend:\", keras.config.backend(), \"| Torch:\", torch.__version__)\n",
    "\n",
    "''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2706210-edf0-4283-96ff-6e5ebe285743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LSTM] #features usadas: 50 → ['transactions', 'revenue', 'avg_price', 'first_sale_hour', 'last_sale_hour', 'avg_sale_hour', 'year', 'month', 'day', 'dayofweek', 'quarter', 'week_of_year', 'is_weekend', 'is_month_start', 'is_month_end'] ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>sMAPE</th>\n",
       "      <th>COV_p10_p90_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.266681</td>\n",
       "      <td>1.592759</td>\n",
       "      <td>65.363569</td>\n",
       "      <td>92.152536</td>\n",
       "      <td>9.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.047509</td>\n",
       "      <td>1.366973</td>\n",
       "      <td>59.251718</td>\n",
       "      <td>92.953566</td>\n",
       "      <td>25.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.043011</td>\n",
       "      <td>1.345018</td>\n",
       "      <td>63.848086</td>\n",
       "      <td>73.409654</td>\n",
       "      <td>25.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.988439</td>\n",
       "      <td>1.289890</td>\n",
       "      <td>53.271689</td>\n",
       "      <td>78.685434</td>\n",
       "      <td>21.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.523825</td>\n",
       "      <td>2.224254</td>\n",
       "      <td>54.315887</td>\n",
       "      <td>102.502781</td>\n",
       "      <td>15.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.891389</td>\n",
       "      <td>1.191235</td>\n",
       "      <td>50.492631</td>\n",
       "      <td>126.473485</td>\n",
       "      <td>15.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.754064</td>\n",
       "      <td>0.997930</td>\n",
       "      <td>55.115054</td>\n",
       "      <td>117.563495</td>\n",
       "      <td>37.500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        MAE      RMSE       MAPE       sMAPE  COV_p10_p90_%\n",
       "1  1.266681  1.592759  65.363569   92.152536          9.375\n",
       "2  1.047509  1.366973  59.251718   92.953566         25.000\n",
       "3  1.043011  1.345018  63.848086   73.409654         25.000\n",
       "4  0.988439  1.289890  53.271689   78.685434         21.875\n",
       "5  1.523825  2.224254  54.315887  102.502781         15.625\n",
       "6  0.891389  1.191235  50.492631  126.473485         15.625\n",
       "7  0.754064  0.997930  55.115054  117.563495         37.500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>sMAPE</th>\n",
       "      <th>COV_p10_p90_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.07356</td>\n",
       "      <td>1.475543</td>\n",
       "      <td>57.877748</td>\n",
       "      <td>97.677279</td>\n",
       "      <td>21.428571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       MAE      RMSE       MAPE      sMAPE  COV_p10_p90_%\n",
       "0  1.07356  1.475543  57.877748  97.677279      21.428571"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mlflow] warning leyendo overall 'results\\lstm_direct_metrics_overall.csv': Invalid metric name: 'COV_p10_p90_%'. Names may only contain alphanumerics, underscores (_), dashes (-), periods (.), spaces ( ), and slashes (/).\n",
      "[mlflow] warning leyendo by_h 'results\\lstm_direct_metrics_by_h.csv': Invalid metric name: 'COV_p10_p90_%_by_h'. Names may only contain alphanumerics, underscores (_), dashes (-), periods (.), spaces ( ), and slashes (/).\n"
     ]
    }
   ],
   "source": [
    "#21\n",
    "#LSTM (Keras 3 con backend PyTorch) — Multi-salida directa (vector de HORIZON)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "# Hiperparámetros \n",
    "LSTM_LOOKBACK = 30           # días en la ventana de entrada\n",
    "LSTM_UNITS    = 64\n",
    "LSTM_DROPOUT  = 0.2\n",
    "LSTM_EPOCHS   = 50\n",
    "LSTM_BATCH    = 256\n",
    "LSTM_LR       = 1e-3\n",
    "LSTM_PATIENCE = 5\n",
    "\n",
    "\n",
    "\n",
    "if \"TARGET\" not in globals() or not isinstance(TARGET, str) or TARGET not in df.columns:\n",
    "    print(\"[LSTM] Aviso: 'TARGET' inválido o ausente; intentando autodetectar columna objetivo.\")\n",
    "    # Heurística: priorizar nombres típicos; si no, toma la primera numérica con pinta de target\n",
    "    candidatos_target = [c for c in [\"revenue\",\"units\",\"qty\",\"quantity\",\"target\",\"y\"] if c in df.columns]\n",
    "    if candidatos_target:\n",
    "        TARGET = candidatos_target[0]\n",
    "    else:\n",
    "        # fallback: primera numérica distinta de id/fecha\n",
    "        num_cols = [c for c in df.columns\n",
    "                    if c not in {\"date\",\"product\"} and pd.api.types.is_numeric_dtype(df[c])]\n",
    "        if not num_cols:\n",
    "            raise ValueError(\"No se pudo determinar la columna objetivo (no hay columnas numéricas).\")\n",
    "        TARGET = num_cols[0]\n",
    "    print(f\"[LSTM] TARGET restaurado → '{TARGET}'\")\n",
    "\n",
    "\n",
    "df[TARGET] = pd.to_numeric(df[TARGET], errors=\"coerce\")\n",
    "\n",
    "\n",
    "\n",
    "def _numeric_feature_cols(dfin: pd.DataFrame, target: str) -> List[str]:\n",
    "    \"\"\"Selecciona columnas numéricas útiles: excluye 'date','product' y futuros (y_*).\"\"\"\n",
    "    cols = [c for c in dfin.columns\n",
    "            if c not in {\"date\",\"product\"}\n",
    "            and not c.startswith(\"y_\")\n",
    "            and pd.api.types.is_numeric_dtype(dfin[c])]\n",
    "    # Asegura que el TARGET histórico esté presente como canal\n",
    "    if target not in cols and target in dfin.columns:\n",
    "        cols = [target] + cols\n",
    "    return cols\n",
    "\n",
    "def _build_lstm_direct_model(n_feats: int, horizon: int):\n",
    "    inp = layers.Input(shape=(LSTM_LOOKBACK, n_feats))\n",
    "    x = layers.LSTM(LSTM_UNITS, return_sequences=False)(inp)\n",
    "    x = layers.Dropout(LSTM_DROPOUT)(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    out = layers.Dense(horizon)(x)   # vector [y+1 .. y+H]\n",
    "    m = keras.Model(inp, out)\n",
    "    m.compile(optimizer=keras.optimizers.Adam(learning_rate=LSTM_LR), loss=\"mse\")\n",
    "    return m\n",
    "\n",
    "# MC Dropout para intervalos P10–P90 (cobertura)\n",
    "\n",
    "def _to_numpy(x):\n",
    "    \"\"\"Convierte salidas Keras (torch/tf) a numpy de forma robusta.\"\"\"\n",
    "    try:\n",
    "        \n",
    "        if hasattr(x, \"detach\"):\n",
    "            return x.detach().cpu().numpy()\n",
    "        \n",
    "        return x.numpy()\n",
    "    except Exception:\n",
    "        return np.array(x)\n",
    "\n",
    "def mc_dropout_predict_vec(model, X_win, n_samples=50):\n",
    "    \"\"\"\n",
    "    Realiza N pases con dropout activo y devuelve:\n",
    "    mean, p10, p90  (vectores de tamaño 'horizon').\n",
    "    (soporta backend torch/tf)\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    for _ in range(n_samples):\n",
    "        y = model(X_win, training=True)  # activa dropout en inferencia\n",
    "        y = _to_numpy(y).ravel()         \n",
    "        preds.append(y)\n",
    "    P = np.stack(preds, axis=0)  # (n_samples, HORIZON)\n",
    "    mean = P.mean(axis=0)\n",
    "    p10  = np.percentile(P, 10, axis=0)\n",
    "    p90  = np.percentile(P, 90, axis=0)\n",
    "    return mean, p10, p90\n",
    "\n",
    "\n",
    "def _make_supervised_direct(df_train: pd.DataFrame, feats: List[str], target: str, lookback: int, horizon: int\n",
    "                           ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Construye dataset (X, Y) para entrenamiento directo multi-horizonte.\n",
    "    X: (n_samples, lookback, n_feats)\n",
    "    Y: (n_samples, horizon) con el target de t+1..t+H\n",
    "    \"\"\"\n",
    "    Xs, Ys = [], []\n",
    "    for _, g in df_train.sort_values([\"product\",\"date\"]).groupby(\"product\"):\n",
    "        g = g.copy()\n",
    "        g[feats] = g[feats].ffill().bfill()\n",
    "        vals = g[feats].values\n",
    "        tgt  = g[target].values.astype(float)\n",
    "\n",
    "        # Último índice i que permite horizonte completo: i <= len(g) - horizon - 1\n",
    "        for i in range(lookback - 1, len(g) - horizon):\n",
    "            Xs.append(vals[i - lookback + 1 : i + 1, :])\n",
    "            Ys.append(tgt[i + 1 : i + 1 + horizon])\n",
    "    if not Xs:\n",
    "        return np.empty((0, lookback, len(feats))), np.empty((0, horizon))\n",
    "    return np.stack(Xs), np.stack(Ys)\n",
    "\n",
    "def _infer_direct_for_split(model, scaler, feats, train_prod, test_prod, target: str, lookback: int, horizon: int):\n",
    "    \"\"\"\n",
    "    Para UN producto y UN split: usa la ÚLTIMA ventana del train para predecir\n",
    "    los HORIZON días del bloque de test (test_start..test_end).\n",
    "    \"\"\"\n",
    "    gtr = train_prod.sort_values(\"date\").copy()\n",
    "    gte = test_prod.sort_values(\"date\").copy()\n",
    "\n",
    "    # Asegura features disponibles\n",
    "    gtr[feats] = gtr[feats].ffill().bfill()\n",
    "    gte[feats] = gte[feats].ffill().bfill()\n",
    "\n",
    "    # Construir la última ventana del train (si falta historia, prepad con la primera fila)\n",
    "    hist_vals = gtr[feats].values\n",
    "    if len(hist_vals) == 0:\n",
    "        # Sin historia rellena con las primeras filas del test (caso extremo)\n",
    "        hist_vals = gte[feats].values[:1, :]\n",
    "    if len(hist_vals) < lookback:\n",
    "        prepad = np.repeat(hist_vals[[0], :], lookback - len(hist_vals), axis=0)\n",
    "        window = np.vstack([prepad, hist_vals])\n",
    "    else:\n",
    "        window = hist_vals[-lookback:, :]\n",
    "\n",
    "    # Escalar e inferir\n",
    "    X_win = scaler.transform(window)  # scaler espera 2D; transform por filas\n",
    "    X_win = X_win.reshape(1, lookback, len(feats)).astype(\"float32\")\n",
    "\n",
    "    # Predicción con incertidumbre por MC Dropout\n",
    "    yhat_vec, p10_vec, p90_vec = mc_dropout_predict_vec(model, X_win, n_samples=50)\n",
    "\n",
    "    # Mapear fechas de test a yhat[h-1]\n",
    "    out = gte[[\"date\"]].copy()\n",
    "    out[\"h\"] = (out[\"date\"] - gte[\"date\"].min()).dt.days + 1  # 1..HORIZON\n",
    "    out[\"yhat\"]     = [yhat_vec[h-1] if 1 <= h <= horizon else np.nan for h in out[\"h\"]]\n",
    "    out[\"yhat_p10\"] = [p10_vec[h-1]  if 1 <= h <= horizon else np.nan for h in out[\"h\"]]\n",
    "    out[\"yhat_p90\"] = [p90_vec[h-1]  if 1 <= h <= horizon else np.nan for h in out[\"h\"]]\n",
    "    out[\"product\"]  = gte[\"product\"].iloc[0]\n",
    "    return out[[\"date\",\"product\",\"h\",\"yhat\",\"yhat_p10\",\"yhat_p90\"]]\n",
    "\n",
    "#  Entrenamiento + backtesting \n",
    "\n",
    "# Comprobación de backend NN disponible\n",
    "try:\n",
    "    _ = keras.config.backend()\n",
    "    TF_OK = True\n",
    "except Exception as e:\n",
    "    print(\"[LSTM-direct] Backend Keras no disponible, se omite. Error:\", e)\n",
    "    TF_OK = False\n",
    "\n",
    "lstm_dir_all = []\n",
    "if TF_OK:\n",
    "    # Definir features base: usa 'candidates' si existen; si no, toma numéricos razonables\n",
    "    try:\n",
    "        base_feats = candidates.copy()\n",
    "        if TARGET not in base_feats:\n",
    "            base_feats = [TARGET] + base_feats\n",
    "    except NameError:\n",
    "        base_feats = _numeric_feature_cols(df, TARGET)\n",
    "    #  base_feats: dejar solo nombres de columnas numéricas válidas\n",
    "\n",
    "    bad_items = [c for c in base_feats if not isinstance(c, str)]\n",
    "    if bad_items:\n",
    "        print(\"[LSTM] Aviso: removiendo items no-string en features:\", bad_items)\n",
    "    \n",
    "\n",
    "    base_feats = [c for c in base_feats\n",
    "                  if isinstance(c, str)\n",
    "                  and c in df.columns\n",
    "                  and pd.api.types.is_numeric_dtype(df[c])]\n",
    "    \n",
    "  \n",
    "    if TARGET in df.columns and TARGET not in base_feats:\n",
    "        base_feats = [TARGET] + base_feats\n",
    "    \n",
    "    \n",
    "    if not base_feats:\n",
    "        print(\"[LSTM] Aviso: 'base_feats' vacío tras limpiar; usando _numeric_feature_cols(df, TARGET).\")\n",
    "        base_feats = _numeric_feature_cols(df, TARGET)\n",
    "    \n",
    "    print(f\"[LSTM] #features usadas: {len(base_feats)} → {base_feats[:15]}{' ...' if len(base_feats)>15 else ''}\")\n",
    "\n",
    "    splits = rolling_origins(df[\"date\"], n_origins=N_ORIGINS, horizon=HORIZON)\n",
    "\n",
    "    for (train_end, test_start, test_end) in splits:\n",
    "        train = df[df[\"date\"] <= train_end].copy()\n",
    "        test  = df[(df[\"date\"] >= test_start) & (df[\"date\"] <= test_end)].copy()\n",
    "\n",
    "        if train[\"date\"].nunique() < MIN_TRAIN_DAYS:\n",
    "            print(f\"[LSTM-direct] Split saltado por historia insuficiente ({train['date'].nunique()} días).\")\n",
    "            continue\n",
    "\n",
    "        # Preparar scaler sobre TRAIN únicamente\n",
    "        df_train_ff = train.sort_values([\"product\",\"date\"]).copy()\n",
    "        df_train_ff[base_feats] = df_train_ff[base_feats].ffill().bfill()\n",
    "        scaler = StandardScaler().fit(df_train_ff[base_feats].values)\n",
    "\n",
    "        # Dataset supervisado directo (solo TRAIN  sin fuga)\n",
    "        X_tr, Y_tr = _make_supervised_direct(df_train_ff, base_feats, TARGET, LSTM_LOOKBACK, HORIZON)\n",
    "        if X_tr.shape[0] == 0:\n",
    "            print(\"[LSTM-direct] No se pudieron construir secuencias en este split.\")\n",
    "            continue\n",
    "\n",
    "        # Escalar X  (reshape para usar scaler 2D)\n",
    "        \n",
    "        X_tr_2d = X_tr.reshape(-1, X_tr.shape[-1])\n",
    "        X_tr_scaled = scaler.transform(X_tr_2d).reshape(X_tr.shape).astype(\"float32\")  \n",
    "        Y_tr_model  = (np.log1p(Y_tr) if USE_LOG1P_TARGET else Y_tr).astype(\"float32\") \n",
    "\n",
    "\n",
    "        # Modelo y entrenamiento\n",
    "        model = _build_lstm_direct_model(n_feats=len(base_feats), horizon=HORIZON)\n",
    "        es = keras.callbacks.EarlyStopping(monitor=\"loss\", patience=LSTM_PATIENCE, restore_best_weights=True)\n",
    "        model.fit(X_tr_scaled, Y_tr_model, epochs=LSTM_EPOCHS, batch_size=LSTM_BATCH, verbose=0, callbacks=[es])\n",
    "\n",
    "        # Inferencia para el bloque de test (H días) por producto \n",
    "        fold_parts = []\n",
    "        for prod, gte in test.groupby(\"product\"):\n",
    "            gtr = train[train[\"product\"] == prod].copy()\n",
    "            pred_df = _infer_direct_for_split(model, scaler, base_feats, gtr, gte, TARGET, LSTM_LOOKBACK, HORIZON)\n",
    "\n",
    "            # Inversión del log si aplica (incluye bandas)\n",
    "            if USE_LOG1P_TARGET:\n",
    "                for c in [\"yhat\",\"yhat_p10\",\"yhat_p90\"]:\n",
    "                    if c in pred_df.columns:\n",
    "                        pred_df[c] = np.expm1(pred_df[c])\n",
    "\n",
    "            # Mezclar con el target real\n",
    "            merged = gte[[\"date\",\"product\",TARGET]].merge(pred_df, on=[\"date\",\"product\"], how=\"left\")\n",
    "            fold_parts.append(merged)\n",
    "\n",
    "        merged_fold = (pd.concat(fold_parts, ignore_index=True) if fold_parts\n",
    "                       else pd.DataFrame(columns=[\"date\",\"product\",TARGET,\"h\",\"yhat\",\"yhat_p10\",\"yhat_p90\"]))\n",
    "        merged_fold[\"model\"] = \"lstm_direct\"\n",
    "        lstm_dir_all.append(merged_fold[[\"date\",\"product\",TARGET,\"h\",\"yhat\",\"yhat_p10\",\"yhat_p90\",\"model\"]])\n",
    "\n",
    "# Consolidar resultados y métricas\n",
    "lstm_direct_results = (pd.concat(lstm_dir_all, ignore_index=True)\n",
    "                       if lstm_dir_all\n",
    "                       else pd.DataFrame(columns=[\"date\",\"product\",TARGET,\"h\",\"yhat\",\"yhat_p10\",\"yhat_p90\",\"model\"]))\n",
    "\n",
    "by_h_lstm_dir, overall_lstm_dir = summarize_metrics(lstm_direct_results.rename(columns={TARGET:\"y\"}))\n",
    "display(by_h_lstm_dir)\n",
    "display(pd.DataFrame([overall_lstm_dir]))\n",
    "\n",
    "# Guardar resultados\n",
    "RESULTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "lstm_direct_results.to_csv(RESULTS_DIR / \"lstm_direct_forecasts.csv\", index=False)\n",
    "lstm_direct_results_norm = _normalize_for_forecast_save(lstm_direct_results, TARGET)\n",
    "lstm_direct_results_norm.to_csv(RESULTS_DIR / \"lstm_direct_forecasts.csv\", index=False)\n",
    "by_h_lstm_dir.to_csv(RESULTS_DIR / \"lstm_direct_metrics_by_h.csv\", index=False)\n",
    "pd.DataFrame([overall_lstm_dir]).to_csv(RESULTS_DIR / \"lstm_direct_metrics_overall.csv\", index=False)\n",
    "\n",
    "\n",
    "_mlflow_log_run(\n",
    "    model_name=\"lstm_direct\",\n",
    "    params={\n",
    "        \"HORIZON\": HORIZON,\n",
    "        \"LOOKBACK\": LSTM_LOOKBACK,\n",
    "        \"UNITS\": LSTM_UNITS,\n",
    "        \"DROPOUT\": LSTM_DROPOUT,\n",
    "        \"LR\": LSTM_LR,\n",
    "        \"BATCH\": LSTM_BATCH,\n",
    "        \"USE_LOG1P_TARGET\": USE_LOG1P_TARGET\n",
    "    },\n",
    "    overall_csv=RESULTS_DIR / \"lstm_direct_metrics_overall.csv\",\n",
    "    by_h_csv=RESULTS_DIR / \"lstm_direct_metrics_by_h.csv\",\n",
    "    extra_artifacts=[\n",
    "        RESULTS_DIR / \"lstm_direct_forecasts.csv\",\n",
    "        \n",
    "    ],\n",
    "    tags={\"family\": \"lstm\", \"stage\": \"train\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71c3b263-d5a4-4d28-a4b9-91e907b5eb66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>sMAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.507440</td>\n",
       "      <td>2.175300</td>\n",
       "      <td>65.109435</td>\n",
       "      <td>74.592093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.138393</td>\n",
       "      <td>1.729410</td>\n",
       "      <td>61.355219</td>\n",
       "      <td>76.410945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.209821</td>\n",
       "      <td>1.719919</td>\n",
       "      <td>60.912698</td>\n",
       "      <td>82.363135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.209821</td>\n",
       "      <td>1.747386</td>\n",
       "      <td>62.577839</td>\n",
       "      <td>80.546706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.534226</td>\n",
       "      <td>2.322207</td>\n",
       "      <td>61.316672</td>\n",
       "      <td>77.595400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.944940</td>\n",
       "      <td>1.385435</td>\n",
       "      <td>74.751984</td>\n",
       "      <td>91.046764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.800595</td>\n",
       "      <td>1.127878</td>\n",
       "      <td>74.603175</td>\n",
       "      <td>92.678669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   h       MAE      RMSE       MAPE      sMAPE\n",
       "0  1  1.507440  2.175300  65.109435  74.592093\n",
       "1  2  1.138393  1.729410  61.355219  76.410945\n",
       "2  3  1.209821  1.719919  60.912698  82.363135\n",
       "3  4  1.209821  1.747386  62.577839  80.546706\n",
       "4  5  1.534226  2.322207  61.316672  77.595400\n",
       "5  6  0.944940  1.385435  74.751984  91.046764\n",
       "6  7  0.800595  1.127878  74.603175  92.678669"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>sMAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.192177</td>\n",
       "      <td>1.785595</td>\n",
       "      <td>64.930023</td>\n",
       "      <td>82.176245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        MAE      RMSE       MAPE      sMAPE\n",
       "0  1.192177  1.785595  64.930023  82.176245"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#22 — Baselines\n",
    "\n",
    "baseline_all = []\n",
    "splits = rolling_origins(df[\"date\"], n_origins=N_ORIGINS, horizon=HORIZON)\n",
    "\n",
    "for (train_end, test_start, test_end) in splits:\n",
    "    train = df[df[\"date\"] <= train_end].copy()\n",
    "    test  = df[(df[\"date\"] >= test_start) & (df[\"date\"] <= test_end)].copy()\n",
    "\n",
    "    \n",
    "    test[\"date\"] = pd.to_datetime(test[\"date\"])\n",
    "\n",
    "    if train[\"date\"].nunique() < MIN_TRAIN_DAYS:\n",
    "        print(f\"[Baselines] Split saltado por historia insuficiente ({train['date'].nunique()} días).\")\n",
    "        continue\n",
    "\n",
    "    horizon_idx = pd.date_range(test_start, test_end, freq=\"D\")\n",
    "\n",
    "    for name in BASELINE_NAMES:\n",
    "        preds = _baseline_predict(train, horizon_idx, HORIZON, TARGET, name).copy()\n",
    "\n",
    "        \n",
    "        if \"date\" in preds:\n",
    "            preds[\"date\"] = pd.to_datetime(preds[\"date\"])\n",
    "        else:\n",
    "            raise ValueError(f\"[Baselines] {_baseline_predict.__name__} debe devolver columna 'date'.\")\n",
    "\n",
    "        if \"product\" not in preds.columns:\n",
    "            raise ValueError(f\"[Baselines] {_baseline_predict.__name__} debe devolver columna 'product'.\")\n",
    "\n",
    "        if \"yhat\" not in preds.columns:\n",
    "            raise ValueError(f\"[Baselines] {_baseline_predict.__name__} debe devolver columna 'yhat'.\")\n",
    "\n",
    "        \n",
    "        if \"h\" not in preds.columns:\n",
    "            preds[\"h\"] = (preds[\"date\"] - test_start).dt.days + 1  # HORIZON\n",
    "\n",
    "        # Merge con el bloque de test\n",
    "        merged = test.merge(preds[[\"date\",\"product\",\"h\",\"yhat\"]], on=[\"date\",\"product\"], how=\"left\")\n",
    "        merged[\"model\"] = name\n",
    "\n",
    "        baseline_all.append(merged[[\"date\",\"product\",TARGET,\"h\",\"yhat\",\"model\"]])\n",
    "\n",
    "# Resultados agregados (sin bandas cobertura quedará NaN en summarize_metrics)\n",
    "baseline_results = (\n",
    "    pd.concat(baseline_all, ignore_index=True)\n",
    "    if baseline_all else\n",
    "    pd.DataFrame(columns=[\"date\",\"product\",TARGET,\"h\",\"yhat\",\"model\"])\n",
    ")\n",
    "\n",
    "# Métricas por horizonte y global\n",
    "by_h_baseline, overall_baseline = summarize_metrics(baseline_results.rename(columns={TARGET:\"y\"}))\n",
    "\n",
    "\n",
    "if \"h\" not in by_h_baseline.columns:\n",
    "    by_h_baseline = by_h_baseline.reset_index()\n",
    "    if \"index\" in by_h_baseline.columns and \"h\" not in by_h_baseline.columns:\n",
    "        by_h_baseline = by_h_baseline.rename(columns={\"index\": \"h\"})\n",
    "\n",
    "\n",
    "if \"h\" in by_h_baseline.columns:\n",
    "    by_h_baseline = by_h_baseline.sort_values(\"h\").reset_index(drop=True)\n",
    "\n",
    "display(by_h_baseline)\n",
    "display(pd.DataFrame([overall_baseline]))\n",
    "\n",
    "# Guardar\n",
    "RESULTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "baseline_results.to_csv(RESULTS_DIR / \"baselines_forecasts.csv\", index=False)\n",
    "baseline_results_norm = _normalize_for_forecast_save(baseline_results, TARGET)\n",
    "baseline_results_norm.to_csv(RESULTS_DIR / \"baselines_forecasts.csv\", index=False)\n",
    "by_h_baseline.to_csv(RESULTS_DIR / \"baselines_metrics_by_h.csv\", index=False)   \n",
    "pd.DataFrame([overall_baseline]).to_csv(RESULTS_DIR / \"baselines_metrics_overall.csv\", index=False)\n",
    "\n",
    "_mlflow_log_run(\n",
    "    model_name=\"baselines\",\n",
    "    params={\"names\": \",\".join(BASELINE_NAMES), \"HORIZON\": HORIZON},\n",
    "    overall_csv=RESULTS_DIR / \"baselines_metrics_overall.csv\",\n",
    "    by_h_csv=RESULTS_DIR / \"baselines_metrics_by_h.csv\",\n",
    "    extra_artifacts=[RESULTS_DIR / \"baselines_forecasts.csv\"],\n",
    "    tags={\"family\": \"baselines\", \"stage\": \"train\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "634c3265-e46f-4fd5-8c45-ea7635642f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando carpeta de resultados: C:\\Users\\Julian\\results\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Ranking overall de modelos (ordenado por sMAPE; menor = mejor)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>sMAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ensemble</td>\n",
       "      <td>4.849966</td>\n",
       "      <td>16.537163</td>\n",
       "      <td>47.048229</td>\n",
       "      <td>70.255118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all_models</td>\n",
       "      <td>37.739537</td>\n",
       "      <td>56.345040</td>\n",
       "      <td>64.930023</td>\n",
       "      <td>82.176245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>baselines</td>\n",
       "      <td>1.192177</td>\n",
       "      <td>1.785595</td>\n",
       "      <td>64.930023</td>\n",
       "      <td>82.176245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lstm_direct</td>\n",
       "      <td>1.073560</td>\n",
       "      <td>1.475543</td>\n",
       "      <td>57.877748</td>\n",
       "      <td>97.677279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lgbm_direct</td>\n",
       "      <td>1.065964</td>\n",
       "      <td>1.473067</td>\n",
       "      <td>55.987553</td>\n",
       "      <td>98.048288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sarimax</td>\n",
       "      <td>1.020857</td>\n",
       "      <td>1.451967</td>\n",
       "      <td>52.824646</td>\n",
       "      <td>98.716511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>prophet</td>\n",
       "      <td>1.125840</td>\n",
       "      <td>1.579511</td>\n",
       "      <td>50.612771</td>\n",
       "      <td>99.075611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         model        MAE       RMSE       MAPE      sMAPE\n",
       "2     ensemble   4.849966  16.537163  47.048229  70.255118\n",
       "0   all_models  37.739537  56.345040  64.930023  82.176245\n",
       "1    baselines   1.192177   1.785595  64.930023  82.176245\n",
       "4  lstm_direct   1.073560   1.475543  57.877748  97.677279\n",
       "3  lgbm_direct   1.065964   1.473067  55.987553  98.048288\n",
       "6      sarimax   1.020857   1.451967  52.824646  98.716511\n",
       "5      prophet   1.125840   1.579511  50.612771  99.075611"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Ganadores por producto×h (según sMAPE)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product</th>\n",
       "      <th>h</th>\n",
       "      <th>sMAPE</th>\n",
       "      <th>family</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Americano</td>\n",
       "      <td>1</td>\n",
       "      <td>22.865162</td>\n",
       "      <td>prophet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Americano</td>\n",
       "      <td>2</td>\n",
       "      <td>30.205933</td>\n",
       "      <td>lstm_direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Americano</td>\n",
       "      <td>3</td>\n",
       "      <td>30.600267</td>\n",
       "      <td>sarimax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Americano</td>\n",
       "      <td>4</td>\n",
       "      <td>20.684168</td>\n",
       "      <td>lgbm_direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Americano</td>\n",
       "      <td>5</td>\n",
       "      <td>39.391788</td>\n",
       "      <td>sarimax</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     product  h      sMAPE       family\n",
       "0  Americano  1  22.865162      prophet\n",
       "1  Americano  2  30.205933  lstm_direct\n",
       "2  Americano  3  30.600267      sarimax\n",
       "3  Americano  4  20.684168  lgbm_direct\n",
       "4  Americano  5  39.391788      sarimax"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Ensemble – métricas por horizonte (h)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>sMAPE</th>\n",
       "      <th>COV_p10_p90_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.743360</td>\n",
       "      <td>1.233965</td>\n",
       "      <td>39.207437</td>\n",
       "      <td>52.905313</td>\n",
       "      <td>54.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.642904</td>\n",
       "      <td>1.007704</td>\n",
       "      <td>44.140995</td>\n",
       "      <td>68.572092</td>\n",
       "      <td>60.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.798277</td>\n",
       "      <td>1.072999</td>\n",
       "      <td>47.376331</td>\n",
       "      <td>75.522031</td>\n",
       "      <td>64.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.676207</td>\n",
       "      <td>0.997281</td>\n",
       "      <td>48.338805</td>\n",
       "      <td>60.076528</td>\n",
       "      <td>64.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.792558</td>\n",
       "      <td>1.363131</td>\n",
       "      <td>38.539630</td>\n",
       "      <td>62.269901</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.622781</td>\n",
       "      <td>0.948375</td>\n",
       "      <td>55.687702</td>\n",
       "      <td>84.970868</td>\n",
       "      <td>43.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.486960</td>\n",
       "      <td>0.720894</td>\n",
       "      <td>53.101863</td>\n",
       "      <td>78.878425</td>\n",
       "      <td>85.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        MAE      RMSE       MAPE      sMAPE  COV_p10_p90_%\n",
       "1  0.743360  1.233965  39.207437  52.905313      54.166667\n",
       "2  0.642904  1.007704  44.140995  68.572092      60.000000\n",
       "3  0.798277  1.072999  47.376331  75.522031      64.285714\n",
       "4  0.676207  0.997281  48.338805  60.076528      64.285714\n",
       "5  0.792558  1.363131  38.539630  62.269901      50.000000\n",
       "6  0.622781  0.948375  55.687702  84.970868      43.750000\n",
       "7  0.486960  0.720894  53.101863  78.878425      85.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Ensemble – métricas overall"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>sMAPE</th>\n",
       "      <th>COV_p10_p90_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.669982</td>\n",
       "      <td>1.054812</td>\n",
       "      <td>46.355818</td>\n",
       "      <td>70.021924</td>\n",
       "      <td>60.625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        MAE      RMSE       MAPE      sMAPE  COV_p10_p90_%\n",
       "0  0.669982  1.054812  46.355818  70.021924         60.625"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mlflow] warning leyendo overall 'results\\ensemble_metrics_overall.csv': Invalid metric name: 'COV_p10_p90_%'. Names may only contain alphanumerics, underscores (_), dashes (-), periods (.), spaces ( ), and slashes (/).\n",
      "[mlflow] warning leyendo by_h 'results\\ensemble_metrics_by_h.csv': Invalid metric name: 'COV_p10_p90_%_by_h'. Names may only contain alphanumerics, underscores (_), dashes (-), periods (.), spaces ( ), and slashes (/).\n"
     ]
    }
   ],
   "source": [
    "#22\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "def show_df(title: str, df):\n",
    "    display(Markdown(f\"### {title}\"))\n",
    "    try:\n",
    "        if hasattr(df, \"empty\") and df.empty:\n",
    "            display(Markdown(\"_(sin datos)_\"))\n",
    "        else:\n",
    "            display(df)\n",
    "    except Exception:\n",
    "        display(df)\n",
    "\n",
    "# Guardar métricas por producto×h para todas las familias disponibles\n",
    "def save_metrics_by_product(model_name, df_pred, target=TARGET):\n",
    "    if df_pred is None or df_pred.empty:\n",
    "        return\n",
    "    tmp = df_pred.copy()\n",
    "\n",
    "    \n",
    "    if \"y\" not in tmp.columns and target in tmp.columns:\n",
    "        tmp = tmp.rename(columns={target: \"y\"})\n",
    "\n",
    "    \n",
    "    if \"y\" not in tmp.columns:\n",
    "        print(f\"[save_metrics_by_product] {model_name}: no tiene columna 'y' ni '{target}'. Se omite.\")\n",
    "        return\n",
    "    if \"h\" not in tmp.columns:\n",
    "        \n",
    "        if \"index\" in tmp.columns and \"h\" not in tmp.columns:\n",
    "            tmp = tmp.rename(columns={\"index\": \"h\"})\n",
    "        elif isinstance(tmp.index, pd.MultiIndex) and \"h\" in tmp.index.names:\n",
    "            tmp = tmp.reset_index()\n",
    "        if \"h\" not in tmp.columns:\n",
    "            print(f\"[save_metrics_by_product] {model_name}: no se encontró 'h'. Se omite.\")\n",
    "            return\n",
    "\n",
    "    prod_h = (tmp.groupby([\"product\",\"h\"])\n",
    "                .apply(lambda g: pd.Series({\"MAE\": mae(g[\"y\"], g[\"yhat\"]),\n",
    "                                            \"sMAPE\": smape(g[\"y\"], g[\"yhat\"])}))\n",
    "                .reset_index())\n",
    "    prod_h.to_csv(RESULTS_DIR / f\"{model_name}_metrics_by_product_h.csv\", index=False)\n",
    "\n",
    "save_metrics_by_product(\"lgbm_direct\", lgbm_results)\n",
    "save_metrics_by_product(\"prophet\", locals().get(\"prophet_results\", pd.DataFrame()))\n",
    "save_metrics_by_product(\"sarimax\", sarimax_results)\n",
    "save_metrics_by_product(\"baselines\", baseline_results)\n",
    "save_metrics_by_product(\"lstm_direct\", locals().get(\"lstm_direct_results\", pd.DataFrame()))  \n",
    "\n",
    "# Ranking overall\n",
    "print(\"Usando carpeta de resultados:\", RESULTS_DIR.resolve())\n",
    "paths = list(RESULTS_DIR.glob(\"*_metrics_overall.csv\"))\n",
    "rows = []\n",
    "for p in paths:\n",
    "    try:\n",
    "        dfm = pd.read_csv(p)\n",
    "        if not dfm.empty:\n",
    "            model = p.stem.replace(\"_metrics_overall\", \"\")\n",
    "            row = dfm.iloc[0].to_dict()\n",
    "            row[\"model\"] = model\n",
    "            rows.append(row)\n",
    "    except Exception as e:\n",
    "        print(\"Error leyendo\", p.name, e)\n",
    "\n",
    "if rows:\n",
    "    cols = [\"model\",\"MAE\",\"RMSE\",\"MAPE\",\"sMAPE\"]\n",
    "    rank = pd.DataFrame(rows)[cols].sort_values(\"sMAPE\")\n",
    "    show_df(\"Ranking overall de modelos (ordenado por sMAPE; menor = mejor)\", rank)\n",
    "    rank.to_csv(RESULTS_DIR / \"_model_ranking.csv\", index=False)\n",
    "else:\n",
    "    print(\"No hay métricas overall para comparar.\")\n",
    "\n",
    "# Ensemble por producto: escoger mejor modelo por producto×h según sMAPE del último split (aprox usando todo)\n",
    "def load_family(name):\n",
    "    path = RESULTS_DIR / f\"{name}_forecasts.csv\"\n",
    "    return pd.read_csv(path) if path.exists() else pd.DataFrame()\n",
    "\n",
    "families = {\n",
    "    \"lgbm_direct\": load_family(\"lgbm_direct\"),\n",
    "    \"prophet\": load_family(\"prophet\"),\n",
    "    \"sarimax\": load_family(\"sarimax\"),\n",
    "    \"baselines\": load_family(\"baselines\"),\n",
    "    \"lstm_direct\": load_family(\"lstm_direct\"),   \n",
    "}\n",
    "# Filtrar vacíos\n",
    "families = {k:v for k,v in families.items() if not v.empty}\n",
    "\n",
    "# Calcular sMAPE por product×h por familia y quedarSE con el ganador.\n",
    "scores = []\n",
    "for name, dfp in families.items():\n",
    "    dfp2 = dfp.copy()\n",
    "\n",
    "    \n",
    "    if \"y\" not in dfp2.columns and TARGET in dfp2.columns:\n",
    "        dfp2 = dfp2.rename(columns={TARGET:\"y\"})\n",
    "    if \"y\" not in dfp2.columns:\n",
    "        print(f\"[ensemble] {name}: no tiene 'y' ni '{TARGET}', se omite de scores.\")\n",
    "        continue\n",
    "\n",
    "    \n",
    "    if \"h\" not in dfp2.columns:\n",
    "        if \"index\" in dfp2.columns and \"h\" not in dfp2.columns:\n",
    "            dfp2 = dfp2.rename(columns={\"index\":\"h\"})\n",
    "        elif isinstance(dfp2.index, pd.MultiIndex) and \"h\" in dfp2.index.names:\n",
    "            dfp2 = dfp2.reset_index()\n",
    "        if \"h\" not in dfp2.columns:\n",
    "            print(f\"[ensemble] {name}: no tiene 'h', se omite de scores.\")\n",
    "            continue\n",
    "\n",
    "    sm = (dfp2.groupby([\"product\",\"h\"])\n",
    "              .apply(lambda g: smape(g[\"y\"], g[\"yhat\"]))\n",
    "              .reset_index(name=\"sMAPE\"))\n",
    "    sm[\"family\"] = name\n",
    "    scores.append(sm)\n",
    "\n",
    "if scores:\n",
    "    scores = pd.concat(scores, ignore_index=True)\n",
    "    winners = scores.sort_values([\"product\",\"h\",\"sMAPE\"]).groupby([\"product\",\"h\"]).first().reset_index()\n",
    "    winners.to_csv(RESULTS_DIR / \"winners_by_product_h.csv\", index=False)\n",
    "    show_df(\"Ganadores por producto×h (según sMAPE)\", winners.head())\n",
    "\n",
    "    # Construir las predicciones del ensemble \"winner-takes-all\"\n",
    "    pieces = []\n",
    "    for _, row in winners.iterrows():\n",
    "        fam = row[\"family\"]\n",
    "        prod = row[\"product\"]\n",
    "        h = row[\"h\"]\n",
    "        dfp = families[fam]\n",
    "        sel = dfp[(dfp[\"product\"]==prod) & (dfp[\"h\"]==h)].copy()\n",
    "        if \"model\" not in sel.columns:\n",
    "            sel[\"model\"] = f\"ensemble({fam})\"\n",
    "        pieces.append(sel)\n",
    "    if pieces:\n",
    "        ensemble_preds = pd.concat(pieces, ignore_index=True)\n",
    "        by_h_ens, overall_ens = summarize_metrics(\n",
    "            ensemble_preds.rename(columns={TARGET:\"y\"}) if TARGET in ensemble_preds.columns\n",
    "            else ensemble_preds.rename(columns={\"y\":\"y\"})\n",
    "        )\n",
    "        show_df(\"Ensemble – métricas por horizonte (h)\", by_h_ens)\n",
    "        show_df(\"Ensemble – métricas overall\", pd.DataFrame([overall_ens]))\n",
    "        ensemble_preds.to_csv(RESULTS_DIR / \"ensemble_winner_takes_all.csv\", index=False)\n",
    "        by_h_ens.to_csv(RESULTS_DIR / \"ensemble_metrics_by_h.csv\", index=False)\n",
    "        pd.DataFrame([overall_ens]).to_csv(RESULTS_DIR / \"ensemble_metrics_overall.csv\", index=False)\n",
    "\n",
    "\n",
    "        _mlflow_log_run(\n",
    "    model_name=\"ensemble\",\n",
    "    params={\"strategy\": \"winner-takes-all by (product,h)\"},\n",
    "    overall_csv=RESULTS_DIR / \"ensemble_metrics_overall.csv\",\n",
    "    by_h_csv=RESULTS_DIR / \"ensemble_metrics_by_h.csv\",\n",
    "    extra_artifacts=[RESULTS_DIR / \"ensemble_winner_takes_all.csv\"],\n",
    "    tags={\"family\": \"ensemble\", \"stage\": \"eval\"}\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6fa48b5d-d231-48d3-b56a-e0d1ca904b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: results\\all_models_metrics_overall.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>sMAPE</th>\n",
       "      <th>model</th>\n",
       "      <th>COV_p10_p90_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37.739537</td>\n",
       "      <td>56.345040</td>\n",
       "      <td>64.930023</td>\n",
       "      <td>82.176245</td>\n",
       "      <td>all_models</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22.418259</td>\n",
       "      <td>36.756103</td>\n",
       "      <td>50.460271</td>\n",
       "      <td>72.140813</td>\n",
       "      <td>all_models</td>\n",
       "      <td>54.487179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55.642252</td>\n",
       "      <td>80.267362</td>\n",
       "      <td>96.635683</td>\n",
       "      <td>191.086168</td>\n",
       "      <td>all_models</td>\n",
       "      <td>17.410714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.986557</td>\n",
       "      <td>48.230610</td>\n",
       "      <td>55.772830</td>\n",
       "      <td>98.506430</td>\n",
       "      <td>all_models</td>\n",
       "      <td>7.589286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.251163</td>\n",
       "      <td>49.815489</td>\n",
       "      <td>50.714061</td>\n",
       "      <td>99.912611</td>\n",
       "      <td>all_models</td>\n",
       "      <td>79.910714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32.321921</td>\n",
       "      <td>46.300026</td>\n",
       "      <td>53.800282</td>\n",
       "      <td>99.492605</td>\n",
       "      <td>all_models</td>\n",
       "      <td>82.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.192177</td>\n",
       "      <td>1.785595</td>\n",
       "      <td>64.930023</td>\n",
       "      <td>82.176245</td>\n",
       "      <td>all_models</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.849966</td>\n",
       "      <td>16.537163</td>\n",
       "      <td>47.048229</td>\n",
       "      <td>70.255118</td>\n",
       "      <td>all_models</td>\n",
       "      <td>58.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.065964</td>\n",
       "      <td>1.473067</td>\n",
       "      <td>55.987553</td>\n",
       "      <td>98.048288</td>\n",
       "      <td>all_models</td>\n",
       "      <td>65.178571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>33.986557</td>\n",
       "      <td>48.230610</td>\n",
       "      <td>55.772830</td>\n",
       "      <td>98.506430</td>\n",
       "      <td>all_models</td>\n",
       "      <td>7.589286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.125840</td>\n",
       "      <td>1.579511</td>\n",
       "      <td>50.612771</td>\n",
       "      <td>99.075611</td>\n",
       "      <td>all_models</td>\n",
       "      <td>79.464286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.020859</td>\n",
       "      <td>1.451966</td>\n",
       "      <td>52.824781</td>\n",
       "      <td>98.716658</td>\n",
       "      <td>all_models</td>\n",
       "      <td>81.696429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.192177</td>\n",
       "      <td>1.785595</td>\n",
       "      <td>64.930023</td>\n",
       "      <td>82.176245</td>\n",
       "      <td>baselines</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.669982</td>\n",
       "      <td>1.054812</td>\n",
       "      <td>46.355818</td>\n",
       "      <td>70.021924</td>\n",
       "      <td>ensemble</td>\n",
       "      <td>60.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.065964</td>\n",
       "      <td>1.473067</td>\n",
       "      <td>55.987553</td>\n",
       "      <td>98.048288</td>\n",
       "      <td>lgbm_direct</td>\n",
       "      <td>65.178571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.073560</td>\n",
       "      <td>1.475543</td>\n",
       "      <td>57.877748</td>\n",
       "      <td>97.677279</td>\n",
       "      <td>lstm_direct</td>\n",
       "      <td>21.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.125840</td>\n",
       "      <td>1.579511</td>\n",
       "      <td>50.612771</td>\n",
       "      <td>99.075611</td>\n",
       "      <td>prophet</td>\n",
       "      <td>79.464286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.020857</td>\n",
       "      <td>1.451967</td>\n",
       "      <td>52.824646</td>\n",
       "      <td>98.716511</td>\n",
       "      <td>sarimax</td>\n",
       "      <td>81.696429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          MAE       RMSE       MAPE       sMAPE        model  COV_p10_p90_%\n",
       "0   37.739537  56.345040  64.930023   82.176245   all_models            NaN\n",
       "1   22.418259  36.756103  50.460271   72.140813   all_models      54.487179\n",
       "2   55.642252  80.267362  96.635683  191.086168   all_models      17.410714\n",
       "3   33.986557  48.230610  55.772830   98.506430   all_models       7.589286\n",
       "4   35.251163  49.815489  50.714061   99.912611   all_models      79.910714\n",
       "5   32.321921  46.300026  53.800282   99.492605   all_models      82.142857\n",
       "6    1.192177   1.785595  64.930023   82.176245   all_models            NaN\n",
       "7    4.849966  16.537163  47.048229   70.255118   all_models      58.125000\n",
       "8    1.065964   1.473067  55.987553   98.048288   all_models      65.178571\n",
       "9   33.986557  48.230610  55.772830   98.506430   all_models       7.589286\n",
       "10   1.125840   1.579511  50.612771   99.075611   all_models      79.464286\n",
       "11   1.020859   1.451966  52.824781   98.716658   all_models      81.696429\n",
       "12   1.192177   1.785595  64.930023   82.176245    baselines            NaN\n",
       "13   0.669982   1.054812  46.355818   70.021924     ensemble      60.625000\n",
       "14   1.065964   1.473067  55.987553   98.048288  lgbm_direct      65.178571\n",
       "15   1.073560   1.475543  57.877748   97.677279  lstm_direct      21.428571\n",
       "16   1.125840   1.579511  50.612771   99.075611      prophet      79.464286\n",
       "17   1.020857   1.451967  52.824646   98.716511      sarimax      81.696429"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#23\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "results_dir = Path(\"results\")\n",
    "overall_files = list(results_dir.glob(\"*_metrics_overall.csv\"))\n",
    "\n",
    "dfs = []\n",
    "for f in overall_files:\n",
    "    model_name = f.stem.replace(\"_metrics_overall\",\"\")\n",
    "    df = pd.read_csv(f)\n",
    "    df[\"model\"] = model_name\n",
    "    dfs.append(df)\n",
    "\n",
    "summary = pd.concat(dfs, ignore_index=True)\n",
    "summary.to_csv(results_dir / \"all_models_metrics_overall.csv\", index=False)\n",
    "\n",
    "print(\"Guardado:\", results_dir / \"all_models_metrics_overall.csv\")\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba06e724-5680-4220-82ba-c2ca93ccf6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21.1 — Selección del mejor modelo (overall) y registry\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Configura la métrica de selección (todas se minimizan)\n",
    "BEST_METRIC = \"sMAPE\"      # alternativas: \"RMSE\", \"MAE\"\n",
    "INCLUIR_ENSEMBLE = False   # True si permites que el ensemble sea elegido\n",
    "EXCLUIR = {\"all_models\", \"baselines\"} | (set() if INCLUIR_ENSEMBLE else {\"ensemble\"})  # excluye siempre 'all_models'\n",
    "\n",
    "def _read_overall_metrics(path: Path):\n",
    "    \"\"\"Lee un *_metrics_overall.csv y devuelve dict con métricas principales.\n",
    "       Tolera variaciones de nombre/caso y presencia/ausencia de coverage.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dfm = pd.read_csv(path)\n",
    "        if dfm.empty:\n",
    "            return None\n",
    "        row = dfm.iloc[0]\n",
    "\n",
    "        def pick(*names):\n",
    "            # busca exacto (case-insensitive) y luego por substring\n",
    "            for n in names:\n",
    "                for c in dfm.columns:\n",
    "                    if c.strip().lower() == n.lower():\n",
    "                        return pd.to_numeric(row[c], errors=\"coerce\")\n",
    "            for n in names:\n",
    "                cols = [c for c in dfm.columns if n.lower() in c.lower()]\n",
    "                if cols:\n",
    "                    return pd.to_numeric(row[cols[0]], errors=\"coerce\")\n",
    "            return None\n",
    "\n",
    "        return {\n",
    "            \"MAE\":  pick(\"MAE\"),\n",
    "            \"RMSE\": pick(\"RMSE\"),\n",
    "            \"MAPE\": pick(\"MAPE\"),\n",
    "            \"sMAPE\": pick(\"sMAPE\"),\n",
    "            \"COV_p10_p90_%\": pick(\"COV_p10_p90_%\", \"coverage\", \"cov\")\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"[selector] error leyendo {path.name}: {type(e).__name__}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Descubrir todas las familias disponibles en RESULTS_DIR\n",
    "paths = list(RESULTS_DIR.glob(\"*_metrics_overall.csv\"))\n",
    "rows = []\n",
    "for p in paths:\n",
    "    model_name = p.stem.replace(\"_metrics_overall\", \"\")\n",
    "    if model_name in EXCLUIR:\n",
    "        continue\n",
    "    # (opcional) también puedes excluir baselines aquí si no quieres que compitan:\n",
    "    # if model_name == \"baselines\": continue\n",
    "\n",
    "    m = _read_overall_metrics(p)\n",
    "    if m:\n",
    "        m[\"model\"] = model_name\n",
    "        m[\"path\"] = str(p)\n",
    "        rows.append(m)\n",
    "\n",
    "scores = pd.DataFrame(rows)\n",
    "\n",
    "if not scores.empty:\n",
    "    # Asegurar numéricos\n",
    "    for c in [\"MAE\",\"RMSE\",\"MAPE\",\"sMAPE\",\"COV_p10_p90_%\"]:\n",
    "        if c in scores.columns:\n",
    "            scores[c] = pd.to_numeric(scores[c], errors=\"coerce\")\n",
    "\n",
    "    # Elegir métrica disponible\n",
    "    if BEST_METRIC not in scores.columns or scores[BEST_METRIC].isna().all():\n",
    "        for fallback in [\"sMAPE\",\"RMSE\",\"MAE\"]:\n",
    "            if fallback in scores.columns and not scores[fallback].isna().all():\n",
    "                BEST_METRIC = fallback\n",
    "                break\n",
    "\n",
    "    # Orden (todas son de minimizar)\n",
    "    scores_sorted = scores.sort_values(BEST_METRIC, ascending=True).reset_index(drop=True)\n",
    "\n",
    "    # Mostrar tabla ordenada\n",
    "    cols_show = [c for c in [\"model\",\"MAE\",\"RMSE\",\"MAPE\",\"sMAPE\",\"COV_p10_p90_%\"] if c in scores_sorted.columns]\n",
    "    try:\n",
    "        show_df(f\"📌 Selección de mejor modelo por {BEST_METRIC} (menor = mejor)\", scores_sorted[cols_show])\n",
    "    except Exception:\n",
    "        display(scores_sorted[cols_show])\n",
    "\n",
    "    # Mejor\n",
    "    best_row = scores_sorted.iloc[0].to_dict()\n",
    "    best = {\n",
    "        \"model\": best_row[\"model\"],\n",
    "        \"criteria\": f\"min {BEST_METRIC}\",\n",
    "        \"metrics\": {k: best_row.get(k) for k in [\"MAE\",\"RMSE\",\"MAPE\",\"sMAPE\",\"COV_p10_p90_%\"] if k in scores_sorted.columns},\n",
    "        \"metrics_path\": best_row[\"path\"]\n",
    "    }\n",
    "    print(\"[Mejor modelo por\", BEST_METRIC, \"]:\", best[\"model\"])\n",
    "else:\n",
    "    best = None\n",
    "    print(\"No hay métricas overall para seleccionar mejor modelo.\")\n",
    "\n",
    "# Guardar registry\n",
    "REGISTRY = RESULTS_DIR / \"models\" / \"registry.json\"\n",
    "REGISTRY.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(REGISTRY, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"best_model\": best}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"[Guardado] registry.json ->\", REGISTRY)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858cb5d2-ecdd-4d5e-aadd-1d988512cdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 23 — Final fit & Serialize (según mejor modelo del registry)\n",
    "# 23 — Final fit & Serialize (según mejor modelo del registry)\n",
    "import json, joblib\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "MODELS_DIR = RESULTS_DIR / \"models\"\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- helper: escoger mejor NO-baseline por los CSV overall ----------\n",
    "def _pick_best_non_baseline(results_dir: Path, prefer=\"sMAPE\"):\n",
    "    paths = list(results_dir.glob(\"*_metrics_overall.csv\"))\n",
    "    filas = []\n",
    "    for p in paths:\n",
    "        name = p.stem.replace(\"_metrics_overall\", \"\")\n",
    "        # excluir agregados y baselines/ensemble para despliegue\n",
    "        if name in {\"all_models\", \"baselines\", \"ensemble\"}:\n",
    "            continue\n",
    "        try:\n",
    "            dfm = pd.read_csv(p)\n",
    "            if dfm.empty:\n",
    "                continue\n",
    "            row = dfm.iloc[0]\n",
    "            def pick(col):\n",
    "                # tolera mayúsculas/minúsculas\n",
    "                for c in dfm.columns:\n",
    "                    if c.strip().lower() == col.lower():\n",
    "                        return pd.to_numeric(row[c], errors=\"coerce\")\n",
    "                # búsqueda por substring como fallback\n",
    "                for c in dfm.columns:\n",
    "                    if col.lower() in c.strip().lower():\n",
    "                        return pd.to_numeric(row[c], errors=\"coerce\")\n",
    "                return None\n",
    "            filas.append({\n",
    "                \"model\": name,\n",
    "                \"MAE\": pick(\"MAE\"),\n",
    "                \"RMSE\": pick(\"RMSE\"),\n",
    "                \"sMAPE\": pick(\"sMAPE\"),\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"[final-fit] no se pudo leer {p.name}: {e}\")\n",
    "    if not filas:\n",
    "        return None\n",
    "    s = pd.DataFrame(filas)\n",
    "    # métrica preferida (todas minimizan); cae a RMSE o MAE si falta\n",
    "    metric = prefer if prefer in s.columns else (\"sMAPE\" if \"sMAPE\" in s.columns else (\"RMSE\" if \"RMSE\" in s.columns else \"MAE\"))\n",
    "    s = s.dropna(subset=[metric])\n",
    "    if s.empty:\n",
    "        return None\n",
    "    best_row = s.sort_values(metric, ascending=True).iloc[0]\n",
    "    return str(best_row[\"model\"])\n",
    "\n",
    "# ---------- leer registry y aplicar protección anti-baseline ----------\n",
    "REGISTRY = MODELS_DIR / \"registry.json\"\n",
    "with open(REGISTRY, \"r\", encoding=\"utf-8\") as f:\n",
    "    reg = json.load(f)\n",
    "best = reg.get(\"best_model\") or {}\n",
    "best_name = (best.get(\"model\") or \"\").lower()\n",
    "\n",
    "# Si el registry eligió baseline/ensemble o viene vacío, forzamos el mejor NO-baseline\n",
    "if best_name in {\"\", \"baselines\", \"ensemble\"}:\n",
    "    alt = _pick_best_non_baseline(RESULTS_DIR, prefer=\"sMAPE\")\n",
    "    if alt:\n",
    "        print(f\"[Final-Fit] Ignorando '{best_name}' para despliegue. Usando mejor no-baseline: {alt}\")\n",
    "        best_name = alt\n",
    "    else:\n",
    "        # como último recurso puedes caer en LGBM (si quieres mantener el comportamiento previo)\n",
    "        print(\"[Final-Fit] No hay modelos no-baseline con métricas overall. Se usará 'lgbm_direct' por defecto.\")\n",
    "        best_name = \"lgbm_direct\"\n",
    "\n",
    "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def _save_meta(name, meta):\n",
    "    with open(MODELS_DIR / f\"{name}_{ts}.meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "if best_name in [\"lgbm_direct\", \"\"]:\n",
    "    # ===== Re-entrenar LGBM Directo con TODA la historia =====\n",
    "    # Usamos tus funciones de #12: candidates y build_direct_labels\n",
    "    df_full = df.copy()\n",
    "    df_l_full = build_direct_labels(df_full, TARGET, H=HORIZON)\n",
    "\n",
    "    # (opcional) re-selección de features por importancia con toda la historia:\n",
    "    # usa la función lgbm_importance_until en la última fecha\n",
    "    try:\n",
    "        last_date = df_l_full[\"date\"].max()\n",
    "        imp_h1 = lgbm_importance_until(df_l_full, candidates, \"y_1\", last_date).head(TOPK_IMP)\n",
    "        imp_h7 = lgbm_importance_until(df_l_full, candidates, \"y_7\", last_date).head(TOPK_IMP)\n",
    "        selected_feats = sorted(set(imp_h1[\"feature\"]).union(set(imp_h7[\"feature\"])))\n",
    "    except Exception:\n",
    "        selected_feats = candidates\n",
    "\n",
    "    # Entrena un modelo por cada horizonte y guárdalos en un dict (o 1 multioutput si tienes ese wrapper)\n",
    "    final_models = {}\n",
    "    for h in range(1, HORIZON+1):\n",
    "        y_col = f\"y_{h}\"\n",
    "        tr = df_l_full.dropna(subset=[y_col]).copy()\n",
    "        if tr.empty: \n",
    "            continue\n",
    "        X_tr, y_tr = tr[selected_feats], tr[y_col]\n",
    "        if USE_LOG1P_TARGET:\n",
    "            y_tr = np.log1p(y_tr)\n",
    "        m = _fit_lgbm(X_tr, y_tr, objective=\"auto\")  # usa tu helper de #12\n",
    "        final_models[h] = m\n",
    "\n",
    "    # Serializa el “ensemble” por horizonte\n",
    "    joblib.dump({\"models_by_h\": final_models, \"features\": selected_feats}, MODELS_DIR / f\"lgbm_direct_{ts}.joblib\")\n",
    "    _save_meta(\"lgbm_direct\", {\n",
    "        \"model_type\": \"lgbm_direct\",\n",
    "        \"target\": TARGET,\n",
    "        \"horizon\": int(HORIZON),\n",
    "        \"features\": selected_feats,\n",
    "        \"use_log1p\": bool(USE_LOG1P_TARGET)\n",
    "    })\n",
    "    print(\"[Guardado] lgbm_direct_\", ts)\n",
    "\n",
    "elif best_name == \"lstm_direct\":\n",
    "    # ===== Re-entrenar LSTM Directo con TODA la historia =====\n",
    "    # Reusa la misma receta que en #19\n",
    "    try:\n",
    "        base_feats = candidates.copy()\n",
    "        if TARGET not in base_feats: base_feats = [TARGET] + base_feats\n",
    "    except NameError:\n",
    "        base_feats = [c for c in df.columns if c not in {\"date\",\"product\"} and not c.startswith(\"y_\") and pd.api.types.is_numeric_dtype(df[c])]\n",
    "        if TARGET not in base_feats: base_feats = [TARGET] + base_feats\n",
    "\n",
    "    df_train_ff = df.sort_values([\"product\",\"date\"]).copy()\n",
    "    df_train_ff[base_feats] = df_train_ff[base_feats].ffill().bfill()\n",
    "\n",
    "    scaler = StandardScaler().fit(df_train_ff[base_feats].values)\n",
    "    X_tr, Y_tr = _make_supervised_direct(df_train_ff, base_feats, TARGET, LSTM_LOOKBACK, HORIZON)\n",
    "    X2 = scaler.transform(X_tr.reshape(-1, X_tr.shape[-1])).reshape(X_tr.shape)\n",
    "    Y2 = np.log1p(Y_tr) if USE_LOG1P_TARGET else Y_tr\n",
    "\n",
    "    final_lstm = _build_lstm_direct_model(n_feats=len(base_feats), horizon=HORIZON)\n",
    "    es = keras.callbacks.EarlyStopping(monitor=\"loss\", patience=LSTM_PATIENCE, restore_best_weights=True)\n",
    "    final_lstm.fit(X2, Y2, epochs=LSTM_EPOCHS, batch_size=LSTM_BATCH, verbose=0, callbacks=[es])\n",
    "\n",
    "    # Serializa\n",
    "    final_lstm.save(MODELS_DIR / f\"lstm_direct_{ts}.keras\")\n",
    "    joblib.dump(scaler, MODELS_DIR / f\"lstm_direct_{ts}.scaler.joblib\")\n",
    "    _save_meta(\"lstm_direct\", {\n",
    "        \"model_type\": \"lstm_direct\",\n",
    "        \"target\": TARGET,\n",
    "        \"horizon\": int(HORIZON),\n",
    "        \"lookback\": int(LSTM_LOOKBACK),\n",
    "        \"features\": base_feats,\n",
    "        \"use_log1p\": bool(USE_LOG1P_TARGET)\n",
    "    })\n",
    "    print(\"[Guardado] lstm_direct_\", ts)\n",
    "\n",
    "else:\n",
    "    print(f\"[Info] El mejor en registry es '{best_name}'. Para Prophet/SARIMAX, guarda por producto:\")\n",
    "    print(\" - Prophet: model_to_json(m) por producto\")\n",
    "    print(\" - SARIMAX: results.fit().save(path) por producto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340972b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18\n",
    "meta = {\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()),\n",
    "    \"python\": sys.version,\n",
    "    \"os\": platform.platform(),\n",
    "    \"params\": {\n",
    "        \"TARGET\": TARGET,\n",
    "        \"HORIZON\": HORIZON,\n",
    "        \"N_ORIGINS\": N_ORIGINS,\n",
    "        \"MIN_TRAIN_DAYS\": MIN_TRAIN_DAYS,\n",
    "        \"TOPK_IMP\": TOPK_IMP,\n",
    "        \"USE_LOG1P_TARGET\": USE_LOG1P_TARGET,\n",
    "        \"LGBM_OBJECTIVE\": LGBM_OBJECTIVE,\n",
    "        \"TWEEDIE_POWERS\": TWEEDIE_POWERS,\n",
    "        \"CAP_OUTLIERS\": CAP_OUTLIERS,\n",
    "        \"OUTLIER_Q\": OUTLIER_Q,\n",
    "        \"USE_RICH_CALENDAR\": USE_RICH_CALENDAR,\n",
    "        \"UA_HOLIDAYS_PATH\": UA_HOLIDAYS_PATH,\n",
    "        \"ADD_BUSINESS_AGGREGATES\": ADD_BUSINESS_AGGREGATES,\n",
    "        \"USE_INDEX_WEATHER_HOLIDAYS\": USE_INDEX_WEATHER_HOLIDAYS,\n",
    "        \"WEATHER_AGG\": WEATHER_AGG,\n",
    "        \"HOLIDAY_COL\": HOLIDAY_COL,\n",
    "        \"PROPHET_USE_REGRESSORS\": PROPHET_USE_REGRESSORS,\n",
    "        \"RANDOM_STATE\": RANDOM_STATE\n",
    "    }\n",
    "}\n",
    "\n",
    "def file_sha1(path):\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            import hashlib\n",
    "            return hashlib.sha1(f.read()).hexdigest()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "meta[\"data_sha1\"] = file_sha1(DATA_PATH)\n",
    "meta[\"index_sha1\"] = file_sha1(INDEX_PATH)\n",
    "\n",
    "RESULTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "with open(RESULTS_DIR / \"metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"metadata.json guardado en:\", (RESULTS_DIR / \"metadata.json\").resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7174cfae-248f-4a4b-a8af-0c6023f4d6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend store URI: file:///C:/Users/Julian/results/mlruns\n",
      "MLflow version: 2.14.1\n",
      "Launching: C:\\Users\\Julian\\miniconda3\\envs\\coffee310\\python.exe -m mlflow ui --backend-store-uri file:///C:/Users/Julian/results/mlruns --port 5001 --host 127.0.0.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['C:\\\\Users\\\\Julian\\\\miniconda3\\\\envs\\\\coffee...>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys, subprocess\n",
    "\n",
    "MLFLOW_DIR = (RESULTS_DIR / \"mlruns\").resolve()\n",
    "MLFLOW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "backend = MLFLOW_DIR.as_uri()  # p.ej. file:///C:/Users/Julian/tu-proyecto/results/mlruns\n",
    "print(\"Backend store URI:\", backend)\n",
    "\n",
    "# Verificación rápida de instalación\n",
    "import mlflow\n",
    "print(\"MLflow version:\", mlflow.__version__)\n",
    "\n",
    "# Lanza la UI con el mismo Python del notebook\n",
    "cmd = [sys.executable, \"-m\", \"mlflow\", \"ui\",\n",
    "       \"--backend-store-uri\", backend,\n",
    "       \"--port\", \"5001\",\n",
    "       \"--host\", \"127.0.0.1\"]\n",
    "print(\"Launching:\", \" \".join(cmd))\n",
    "subprocess.Popen(cmd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652b1ad5-cf77-49f4-896b-6daf1b6f1ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a69b5eb-0cdd-44c2-8450-1af7714a419a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b0587c-c49d-4a3f-9917-4b8224e45380",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Coffee (Py3.10)",
   "language": "python",
   "name": "coffee-310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
